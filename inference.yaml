apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: newjeans
  namespace: admin-profile
  annotations:
    sidecar.istio.io/inject: "false"
spec:
  predictor:
    containers:
    - args:
      - --model-id
      - /mnt/models/<<pytorch_job>>
      - --quantize
      - "bitsandbytes-nf4"
      image: ghcr.io/huggingface/text-generation-inference:2.0
      name: kserve-container
      ports:
      - containerPort: 8080
        protocol: TCP
      resources:
        limits:
          cpu: "2"
          memory: 40Gi
          nvidia.com/gpu: "1"
        requests:
          cpu: "2"
          memory: 30Gi
          nvidia.com/gpu: "1"
      volumeMounts:
      - mountPath: /dev/shm
        name: shm
      - mountPath: /mnt/models
        name: model
    maxReplicas: 1
    minReplicas: 1
    volumes:
    - emptyDir:
        medium: Memory
        sizeLimit: 0.5Gi
      name: shm
    - persistentVolumeClaim:
        claimName: <<pvc_name>>
      name: model
