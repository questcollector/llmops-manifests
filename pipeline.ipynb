{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b93328c9-e3b2-40d4-9162-5e5cf394b0ec",
   "metadata": {},
   "source": [
    "# Hello World pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dddda02b-e4e0-42e3-a5d8-53946f16bbbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "\n",
    "@dsl.component(base_image=\"python:3.11\")\n",
    "def hello_world(name:str) -> str:\n",
    "    return f\"hello world\\nnice to meet you {name}\"\n",
    "\n",
    "@dsl.component(base_image=\"python:3.11\")\n",
    "def print_str(hello_str:str):\n",
    "    print(hello_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee79faab-f198-42be-abf1-011c0745a16f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "\n",
    "@dsl.pipeline(name=\"hello-world\")\n",
    "def hello_world_pipeline(name:str):\n",
    "    hello_comp = hello_world(name=name)\n",
    "    print_comp = print_str(hello_str=hello_comp.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1850bba1-eb0f-4b85-b54e-41e9f10e87dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp.compiler import Compiler\n",
    "\n",
    "Compiler().compile(hello_world_pipeline, package_path=\"hello-world.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e996fdb1-044f-43e5-b649-35475c03683c",
   "metadata": {},
   "source": [
    "# Dataset processing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70a8460-67c8-4787-9039-a0a590d5711e",
   "metadata": {},
   "source": [
    "## install kfp-kubernetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afdad677-0341-4635-8325-b88a8c611234",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "pip install -q --no-cache-dir kfp[kubernetes]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42f5f3c-9289-4a22-b42d-babcb061619e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 데이터셋 다운로드 후 검증 컴포넌트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0635542d-8eb3-475a-a415-0b629379a56f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'llmops (Python 3.10.12)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/home/kiyoung/llmops-manifests/llmops/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from kfp import dsl\n",
    "from kfp.dsl import Output, Dataset\n",
    "\n",
    "@dsl.component(base_image=\"python:3.11\", packages_to_install=[\"datasets\"])\n",
    "def save_jsonl(\n",
    "    jsonl_url: str,\n",
    "    output_dataset: Output[Dataset]\n",
    "):\n",
    "    import requests\n",
    "    from datasets import load_dataset\n",
    "\n",
    "    jsonl_file = \"temp.jsonl\"\n",
    "    # fetch jsonl file and save temp file\n",
    "    with requests.get(jsonl_url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(jsonl_file, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "\n",
    "    # load dataset from jsonl\n",
    "    dataset = load_dataset(\"json\", data_files=jsonl_file)\n",
    "    # save jsonl\n",
    "    dataset['train'].to_json(output_dataset.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32bff1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "from kfp.dsl import Input, Output, Dataset\n",
    "\n",
    "@dsl.component(base_image=\"python:3.11\", packages_to_install=[\"datasets\"])\n",
    "def validate_dataset(\n",
    "    jsonl_dataset: Input[Dataset],\n",
    "    valid_dataset: Output[Dataset]\n",
    "):\n",
    "    from datasets import load_dataset\n",
    "    \n",
    "    dataset = load_dataset(\"json\", data_files=jsonl_dataset.path)\n",
    "    \n",
    "    # conversational format validation\n",
    "    def is_chat_template(examples):\n",
    "        messages = examples[\"message\"]\n",
    "        result = []\n",
    "        for message in messages:\n",
    "            checked = 0\n",
    "            for item in message:\n",
    "                if isinstance(item[\"role\"].dtype, str) and isinstance(item[\"content\"], str):\n",
    "                    checked += 1\n",
    "            result.append(len(message) == checked and checked > 0)\n",
    "        return result\n",
    "\n",
    "\n",
    "    feature_set = dataset['train'].features\n",
    "    if \"messages\" in feature_set:\n",
    "        dataset = dataset.filter(is_chat_template, batched=True).select_columns(\"messages\")\n",
    "    elif ({\"prompt\", \"completion\"}.issubset(feature_set) and \n",
    "          feature_set[\"prompt\"].dtype == 'string' and feature_set[\"completion\"].dtype == 'string'):\n",
    "        dataset = dataset.select_columns([\"prompt\", \"completion\"])\n",
    "    else:\n",
    "        raise ValueError(\"data should be conversational format or instruction format\") \n",
    "    dataset['train'].to_json(valid_dataset.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1043bd7c-8b54-4c4d-9c46-a718d4f4d1ee",
   "metadata": {},
   "source": [
    "# Training pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cdf445-e717-4610-b0c1-d85ac6463328",
   "metadata": {},
   "source": [
    "## training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bf3f163e-2da8-4916-83b2-cb8c5850206f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "\n",
    "@dsl.component(base_image=\"python:3.11\", packages_to_install=[\"requests\"])\n",
    "def save_train_func(training_code_url:str) -> str:\n",
    "    import requests\n",
    "\n",
    "    response = requests.get(training_code_url)\n",
    "    training_code = response.content.decode('utf-8')\n",
    "\n",
    "    print(f\"training code:\\n\\n{training_code}\")\n",
    "\n",
    "    return training_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7371d68-a07a-401d-b9cd-554f5c9469d4",
   "metadata": {},
   "source": [
    "## deepspeed config 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9fbeec4d-f8fe-4952-8d87-f265873035ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "\n",
    "@dsl.component(base_image=\"python:3.11\", packages_to_install=[\"requests\"])\n",
    "def json_to_dict(\n",
    "    json_url:str,\n",
    "    extras_to_update:str,\n",
    ") -> str:   \n",
    "    import requests\n",
    "    import json\n",
    "\n",
    "    headers = {\"Content-type\": \"application/json\"}\n",
    "    response = requests.get(json_url, headers=headers)\n",
    "    json_data = response.json()\n",
    "\n",
    "    # add extras\n",
    "    def recursive_update(original, updates):\n",
    "        for key, value in updates.items():\n",
    "            # 둘다 dictionary면 재귀적으로 병합\n",
    "            if isinstance(value, dict) and key in original and isinstance(original[key], dict):\n",
    "                recursive_update(original[key], value)\n",
    "            # 업데이트\n",
    "            else:\n",
    "                original[key] = value\n",
    "    if extras_to_update:\n",
    "        extras_to_update = json.loads(extras_to_update)\n",
    "    \n",
    "    recursive_update(json_data, extras_to_update)\n",
    "\n",
    "    print(f\"json data:\\n\\n{json.dumps(json_data, indent=4)}\")\n",
    "\n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34963a49-2f6f-4825-970e-8cbfab37698a",
   "metadata": {},
   "source": [
    "## PytorchJob 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33816172",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "\n",
    "# disable cache\n",
    "@dsl.component(base_image=\"python:3.11\")\n",
    "def generate_job_id() -> str:\n",
    "    import string\n",
    "    import random\n",
    "\n",
    "    ## create job id\n",
    "    letters_set = string.ascii_lowercase + string.digits\n",
    "    random_list = random.sample(letters_set,5)\n",
    "    job_id = ''.join(random_list)\n",
    "\n",
    "    return job_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf55f5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "from kfp.dsl import Input, Dataset\n",
    "\n",
    "@dsl.component(base_image=\"python:3.11\")\n",
    "def generate_pytorchjob_script(\n",
    "    job_name:str,\n",
    "    job_id:str,\n",
    "    training_code:str,\n",
    "    deepspeed_config: dict,\n",
    "    model_id:str,\n",
    "    train_dataset:Input[Dataset],\n",
    "    eval_dataset:Input[Dataset],\n",
    "    mount_path:str,\n",
    "    batch_size:int,\n",
    "    num_train_epochs:int,\n",
    ") -> str:\n",
    "    import os\n",
    "\n",
    "    train_dataset_uri = train_dataset.uri\n",
    "    eval_dataset_uri = eval_dataset.uri\n",
    "\n",
    "    exec_script = f\"\"\"\n",
    "program_path=$(mktemp -d)\n",
    "read -r -d '' SCRIPT << EOM\\n\n",
    "{training_code}\n",
    "EOM\n",
    "printf \"%s\" \"$SCRIPT\" > $program_path/ephemeral_script.py\n",
    "read -r -d '' SCRIPT << EOM\\n\n",
    "{deepspeed_config}\n",
    "EOM\n",
    "printf \"%s\" \"$SCRIPT\" > $program_path/deepspeed.json\n",
    "torchrun --nnodes $WORLD_SIZE --nproc_per_node <<gpu_cnt>> --master-addr $MASTER_ADDR \\\n",
    "--master-port $MASTER_PORT --node-rank $RANK $program_path/ephemeral_script.py \\\n",
    "--model_path {model_id} --train_dataset_path {train_dataset_uri} --eval_dataset_path {eval_dataset_uri} \\\n",
    "--training_job_name {job_name} --training_job_id {job_id} --mount_path {mount_path} \\\n",
    "--per_device_train_batch_size {batch_size} --per_device_eval_batch_size {batch_size} \\\n",
    "--num_train_epochs {num_train_epochs} --deepspeed $program_path/deepspeed.json \\\n",
    "\"\"\"\n",
    "    print(f\"pytorchjob script:\\n\\n{exec_script}\")\n",
    "    return exec_script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442cd205-0fd9-4290-a0d1-3c8f1414bfde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "\n",
    "@dsl.component(base_image=\"python:3.11\", packages_to_install=[\"kubeflow-training\"])\n",
    "def run_pytorchjob(\n",
    "    job_name:str,\n",
    "    job_id:str,\n",
    "    base_image:str,\n",
    "    pvc_name:str,\n",
    "    mount_path:str,\n",
    "    exec_script:str,\n",
    "    master_replica:int,\n",
    "    master_resources:str,\n",
    "    worker_replica:int,\n",
    "    worker_resources:str,\n",
    ") -> str:\n",
    "    import json\n",
    "    from kubernetes import client\n",
    "    from kubeflow.training.constants import constants\n",
    "    from kubeflow.training import models\n",
    "\n",
    "    # parse node resources\n",
    "    master_resources = json.loads(master_resources)\n",
    "    worker_resources = json.loads(worker_resources)\n",
    "\n",
    "    print(f\"master node resources:\\n{json.dumps(master_resources, indent=4)}\")\n",
    "    print(f\"worker node resources:\\n{json.dumps(worker_resources, indent=4)}\")\n",
    "\n",
    "    # populate nproc_per_node argument\n",
    "    master_exec_script = exec_script.replace(\"<<gpu_cnt>>\", master_resources[\"nvidia.com/gpu\"])\n",
    "    worker_exec_script = exec_script.replace(\"<<gpu_cnt>>\", worker_resources[\"nvidia.com/gpu\"])\n",
    "    \n",
    "    ## declare training pod spec    \n",
    "    pod_template_spec = client.V1PodTemplateSpec(\n",
    "        metadata=client.V1ObjectMeta(annotations={constants.ISTIO_SIDECAR_INJECTION: \"false\"}), ## istio sidecar disable\n",
    "        spec=client.V1PodSpec(\n",
    "            restart_policy=\"Never\",\n",
    "            containers=[\n",
    "                client.V1Container(\n",
    "                    name=constants.PYTORCHJOB_CONTAINER,\n",
    "                    image=base_image,\n",
    "                    command=[\"bash\", \"-c\"],\n",
    "                    args=[master_exec_script],\n",
    "                    resources=client.V1ResourceRequirements(\n",
    "                        limits=master_resources,\n",
    "                        requests=master_resources\n",
    "                    ),\n",
    "                    env=[ ## for mlflow artifact store\n",
    "                        client.V1EnvVar(\n",
    "                            name=\"AWS_ACCESS_KEY_ID\",\n",
    "                            value_from=client.V1EnvVarSource(\n",
    "                                secret_key_ref=client.V1SecretKeySelector(\n",
    "                                    name=\"mlpipeline-minio-artifact\",\n",
    "                                    key=\"accesskey\",\n",
    "                                )\n",
    "                            )\n",
    "                        ),\n",
    "                        client.V1EnvVar(\n",
    "                            name=\"AWS_SECRET_ACCESS_KEY\",\n",
    "                            value_from=client.V1EnvVarSource(\n",
    "                                secret_key_ref=client.V1SecretKeySelector(\n",
    "                                    name=\"mlpipeline-minio-artifact\",\n",
    "                                    key=\"secretkey\",\n",
    "                                )\n",
    "                            )\n",
    "                        ),\n",
    "                    ],\n",
    "                    volume_mounts=[\n",
    "                        client.V1VolumeMount(\n",
    "                            mount_path=mount_path,\n",
    "                            name=pvc_name,\n",
    "                            read_only=False,\n",
    "\n",
    "                        ),\n",
    "                        client.V1VolumeMount(\n",
    "                            mount_path=\"/dev/shm\",\n",
    "                            name=\"dshm\",\n",
    "                            read_only=False,\n",
    "                        ),\n",
    "                    ]\n",
    "                )\n",
    "            ],\n",
    "            volumes=[\n",
    "                client.V1Volume(\n",
    "                    name=pvc_name,\n",
    "                    persistent_volume_claim=client.V1PersistentVolumeClaimVolumeSource(\n",
    "                        claim_name=pvc_name,\n",
    "                        read_only=False,\n",
    "                    )\n",
    "                ),\n",
    "                client.V1Volume(\n",
    "                    name=\"dshm\",\n",
    "                    empty_dir=client.V1EmptyDirVolumeSource(\n",
    "                        medium=\"Memory\",\n",
    "                        size_limit=\"1.0Gi\"\n",
    "                    )\n",
    "                )\n",
    "            ]\n",
    "        )        \n",
    "    )\n",
    "    \n",
    "    ## get namespace\n",
    "    with open(\"/var/run/secrets/kubernetes.io/serviceaccount/namespace\", \"r\") as f:\n",
    "        namespace = f.read()\n",
    "    \n",
    "    ## declare pytorchjob    \n",
    "    pytorchjob_name = f\"{job_name}-{job_id}\"\n",
    "    pytorchjob = models.KubeflowOrgV1PyTorchJob(\n",
    "        api_version=constants.API_VERSION,\n",
    "        kind=constants.PYTORCHJOB_KIND,\n",
    "        metadata=client.V1ObjectMeta(name=pytorchjob_name, namespace=namespace),\n",
    "        spec=models.KubeflowOrgV1PyTorchJobSpec(\n",
    "            run_policy=models.KubeflowOrgV1RunPolicy(clean_pod_policy=None),\n",
    "            pytorch_replica_specs={}\n",
    "        )\n",
    "    )\n",
    "    pytorchjob.spec.pytorch_replica_specs[constants.REPLICA_TYPE_MASTER] = models.KubeflowOrgV1ReplicaSpec(replicas=master_replica, template=pod_template_spec)\n",
    "    \n",
    "    import copy\n",
    "    worker_pod_template_spec = copy.deepcopy(pod_template_spec)\n",
    "    worker_pod_template_spec.spec.containers[0].args = [worker_exec_script]\n",
    "    worker_pod_template_spec.spec.containers[0].resources = client.V1ResourceRequirements(\n",
    "        limits=worker_resources,\n",
    "        requests=worker_resources,\n",
    "    )\n",
    "    pytorchjob.spec.pytorch_replica_specs[constants.REPLICA_TYPE_WORKER] = models.KubeflowOrgV1ReplicaSpec(replicas=worker_replica, template=worker_pod_template_spec)\n",
    "        \n",
    "    \n",
    "    ## create pytorchjob\n",
    "    from kubeflow.training import TrainingClient\n",
    "    training_client = TrainingClient()\n",
    "    training_client.create_job(pytorchjob, namespace=namespace)\n",
    "    \n",
    "    ## wait till Running state\n",
    "    running_pytorchjob = training_client.wait_for_job_conditions(\n",
    "        name=pytorchjob.metadata.name,\n",
    "        namespace=pytorchjob.metadata.namespace,\n",
    "        job_kind=constants.PYTORCHJOB_KIND,\n",
    "        expected_conditions={constants.JOB_CONDITION_RUNNING}\n",
    "    )\n",
    "    \n",
    "    ## log master pod\n",
    "    training_client.get_job_logs(\n",
    "        name=running_pytorchjob.metadata.name,\n",
    "        namespace= running_pytorchjob.metadata.namespace,\n",
    "        job_kind=constants.PYTORCHJOB_KIND,\n",
    "        is_master=True,\n",
    "        follow=True\n",
    "    )\n",
    "    \n",
    "    ## check job is succeeded\n",
    "    training_client.wait_for_job_conditions(\n",
    "        name=running_pytorchjob.metadata.name,\n",
    "        namespace=running_pytorchjob.metadata.namespace,\n",
    "        job_kind=constants.PYTORCHJOB_KIND,\n",
    "        expected_conditions={constants.JOB_CONDITION_SUCCEEDED}\n",
    "    )\n",
    "    return pytorchjob_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00328dbe-3d7c-4526-bd4a-83b1ec531d11",
   "metadata": {},
   "source": [
    "## pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "770cd28e-3e42-4031-832e-29dbca1fa31d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "from kfp import kubernetes\n",
    "\n",
    "@dsl.pipeline(name=\"newjeans-fine-tuning\")\n",
    "def fine_tuning_pipeline(\n",
    "    train_data_url:str=\"https://docs.google.com/uc?export=download&id=1ycN8UktwSiMJ0cWwPXeLVIHJpBnUgEtE&confirm=t\",\n",
    "    eval_data_url:str=\"\",\n",
    "    training_code_url:str=\"\",\n",
    "    deepspeed_config_url:str=\"\",\n",
    "    deepspeed_config_extras:str=\"{}\",\n",
    "    base_image:str=\"asia-northeast3-docker.pkg.dev/silver-bridge-433413-f3/llmops/transformers-pytorch-deepspeed-latest-gpu:deepspeed-v0.15.0-24.06.py3\",\n",
    "    model_id:str=\"unsloth/Meta-Llama-3.1-8B-Instruct\",\n",
    "    job_name:str=\"luckyvicky-finetuning\",\n",
    "    batch_size:int=4,\n",
    "    num_train_epochs:int=10,\n",
    "    master_replica:int=1,\n",
    "    master_resources:str='{\"cpu\":\"2\", \"memory\": \"55Gi\", \"nvidia.com/gpu\": \"1\"}',\n",
    "    worker_replica:int=1,\n",
    "    worker_resources:str='{\"cpu\":\"2\", \"memory\": \"20Gi\", \"nvidia.com/gpu\": \"1\"}'\n",
    "):\n",
    "    # create PVC\n",
    "    pvc = kubernetes.CreatePVC(\n",
    "        # can also use pvc_name instead of pvc_name_suffix to use a pre-existing PVC\n",
    "        pvc_name='newjeans-finetuning-pvc-rwx',\n",
    "        access_modes=['ReadWriteMany'],\n",
    "        size='1024Gi',\n",
    "        storage_class_name='filestore-rwx',\n",
    "    )\n",
    "    pvc.set_caching_options(True)\n",
    "    \n",
    "    mount_path = '/train'\n",
    "    \n",
    "    # get \n",
    "    train_dataset = save_jsonl(\n",
    "        jsonl_url=train_data_url\n",
    "    )\n",
    "    train_dataset.set_display_name(\"train-dataset\")\n",
    "    validate_train_dataset = validate_dataset(\n",
    "        train_dataset.output\n",
    "    )\n",
    "    validate_train_dataset.set_display_name(\"validate-train-dataset\")\n",
    "\n",
    "    eval_dataset = save_jsonl(\n",
    "        jsonl_url=eval_data_url\n",
    "    )\n",
    "    eval_dataset.set_display_name(\"eval-dataset\")\n",
    "    validate_eval_dataset = validate_dataset(\n",
    "        eval_dataset.output\n",
    "    )\n",
    "    validate_eval_dataset.set_display_name(\"validate-eval-dataset\")\n",
    "    \n",
    "    train_func = save_train_func(training_code_url=training_code_url)\n",
    "    \n",
    "    deepspeed_config = json_to_dict(\n",
    "        json_url=deepspeed_config_url,\n",
    "        extras_to_update=deepspeed_config_extras\n",
    "    )\n",
    "    deepspeed_config.set_display_name(\"deepspeed-config\")\n",
    "\n",
    "    job_id = generate_job_id()\n",
    "    job_id.set_caching_options(False)\n",
    "\n",
    "    job_script = generate_pytorchjob_script(\n",
    "        job_name=job_name,\n",
    "        job_id=job_id.output,\n",
    "        training_code=train_func.output,\n",
    "        deepspeed_config=deepspeed_config.output,\n",
    "        model_id=model_id,\n",
    "        train_dataset=validate_train_dataset.output,\n",
    "        eval_dataset=validate_eval_dataset.output,\n",
    "        mount_path=mount_path,\n",
    "        batch_size=batch_size,\n",
    "        num_train_epochs=num_train_epochs\n",
    "    )\n",
    "    \n",
    "    pytorchjob = run_pytorchjob(\n",
    "        job_name=job_name,\n",
    "        job_id=job_id.output,\n",
    "        base_image=base_image,\n",
    "        pvc_name=pvc.outputs['name'],\n",
    "        mount_path=mount_path,\n",
    "        exec_script=job_script.output,\n",
    "        master_replica=master_replica,\n",
    "        master_resources=master_resources,\n",
    "        worker_replica=worker_replica,\n",
    "        worker_resources=worker_resources,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2a1c50bd-0cd0-4b8d-ae68-ba6182da367d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp.compiler import Compiler\n",
    "\n",
    "Compiler().compile(fine_tuning_pipeline, package_path=\"luckyvicky-finetuning.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ad7c1c",
   "metadata": {},
   "source": [
    "# Serving pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5960b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "\n",
    "@dsl.component(base_image=\"python:3.11\", packages_to_install=[\"kserve\"])\n",
    "def run_isvc(\n",
    "    pvc_name: str,\n",
    "    model_id:str,\n",
    "    pytorchjob_name: str,\n",
    "):\n",
    "    from kserve import constants\n",
    "    from kserve import (\n",
    "        V1beta1PredictorSpec,\n",
    "        V1beta1InferenceServiceSpec,\n",
    "        V1beta1InferenceService,\n",
    "    )\n",
    "    from kubernetes import client\n",
    "\n",
    "    ## get namespace\n",
    "    with open(\"/var/run/secrets/kubernetes.io/serviceaccount/namespace\", \"r\") as f:\n",
    "        namespace = f.read()\n",
    "    predictor_spec = V1beta1PredictorSpec(\n",
    "        containers=[\n",
    "            client.V1Container(\n",
    "                args=[\n",
    "                    \"--model\", f\"{model_id}\",\n",
    "                    \"--dtype\", \"bfloat16\",\n",
    "                    \"--max-model-len\", \"16384\",\n",
    "                    \"--quantization\", \"bitsandbytes\",\n",
    "                    \"--load-format\", \"bitsandbytes\",\n",
    "                    \"--enable-lora\",\n",
    "                    \"--lora-modules\", f\"luckyvicky=/mnt/models/{pytorchjob_name}\"\n",
    "                ],\n",
    "                image=\"asia-northeast3-docker.pkg.dev/silver-bridge-433413-f3/llmops/vllm/vllm-openai:v0.5.5-bitsandbytes\",\n",
    "                name=\"kserve-container\",\n",
    "                ports=[\n",
    "                    client.V1ContainerPort(\n",
    "                        container_port=8000,\n",
    "                        protocol=\"TCP\",\n",
    "                    )\n",
    "                ],\n",
    "                resources=client.V1ResourceRequirements(\n",
    "                    limits={\"cpu\": \"2\", \"memory\": \"40Gi\", \"nvidia.com/gpu\": \"1\"},\n",
    "                    requests={\"cpu\": \"2\", \"memory\": \"25Gi\", \"nvidia.com/gpu\": \"1\"},\n",
    "                ),\n",
    "                env=[ ## STORAGE_URI\n",
    "                    client.V1EnvVar(\n",
    "                        name=\"STORAGE_URI\",\n",
    "                        value=f\"pvc://{pvc_name}\"\n",
    "                    ),\n",
    "                ],\n",
    "                volume_mounts=[\n",
    "                    client.V1VolumeMount(\n",
    "                        mount_path=\"/dev/shm\",\n",
    "                        name=\"dshm\",\n",
    "                        read_only=False,\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "        ],\n",
    "        max_replicas=1,\n",
    "        min_replicas=1,\n",
    "        volumes=[\n",
    "            client.V1Volume(\n",
    "                name=\"dshm\",\n",
    "                empty_dir=client.V1EmptyDirVolumeSource(\n",
    "                    medium=\"Memory\", size_limit=\"0.5Gi\"\n",
    "                ),\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    inference_service_spec = V1beta1InferenceServiceSpec(predictor=predictor_spec)\n",
    "    inference_service = V1beta1InferenceService(\n",
    "        api_version=constants.KSERVE_V1BETA1,\n",
    "        kind=constants.KSERVE_KIND,\n",
    "        metadata=client.V1ObjectMeta(\n",
    "            name=\"luckyvicky\", \n",
    "            namespace=namespace, \n",
    "            annotations={\n",
    "                \"sidecar.istio.io/inject\": \"false\",\n",
    "                \"serving.kserve.io/enable-prometheus-scraping\": \"true\"\n",
    "            }),\n",
    "        spec=inference_service_spec,\n",
    "    )\n",
    "\n",
    "    from kserve import KServeClient\n",
    "    kserve_client = KServeClient()\n",
    "    kserve_client.create(inference_service, namespace=namespace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa1251f",
   "metadata": {},
   "source": [
    "## pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e979b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "from kfp import kubernetes\n",
    "\n",
    "@dsl.pipeline(name=\"newjeans-fine-tuning\")\n",
    "def fine_tuning_pipeline(\n",
    "    train_data_url:str=\"https://docs.google.com/uc?export=download&id=1ycN8UktwSiMJ0cWwPXeLVIHJpBnUgEtE&confirm=t\",\n",
    "    eval_data_url:str=\"\",\n",
    "    training_code_url:str=\"\",\n",
    "    deepspeed_config_url:str=\"\",\n",
    "    deepspeed_config_extras:str=\"{}\",\n",
    "    base_image:str=\"asia-northeast3-docker.pkg.dev/silver-bridge-433413-f3/llmops/transformers-pytorch-deepspeed-latest-gpu:deepspeed-v0.15.0-24.06.py3\",\n",
    "    model_id:str=\"unsloth/Meta-Llama-3.1-8B-Instruct\",\n",
    "    job_name:str=\"luckyvicky-finetuning\",\n",
    "    batch_size:int=4,\n",
    "    num_train_epochs:int=10,\n",
    "    master_replica:int=1,\n",
    "    master_resources:str='{\"cpu\":\"2\", \"memory\": \"55Gi\", \"nvidia.com/gpu\": \"1\"}',\n",
    "    worker_replica:int=1,\n",
    "    worker_resources:str='{\"cpu\":\"2\", \"memory\": \"20Gi\", \"nvidia.com/gpu\": \"1\"}'\n",
    "):\n",
    "    # create PVC\n",
    "    pvc = kubernetes.CreatePVC(\n",
    "        # can also use pvc_name instead of pvc_name_suffix to use a pre-existing PVC\n",
    "        pvc_name='newjeans-finetuning-pvc-rwx',\n",
    "        access_modes=['ReadWriteMany'],\n",
    "        size='1024Gi',\n",
    "        storage_class_name='filestore-rwx',\n",
    "    )\n",
    "    pvc.set_caching_options(True)\n",
    "    \n",
    "    mount_path = '/train'\n",
    "    \n",
    "    # get \n",
    "    train_dataset = save_jsonl(\n",
    "        jsonl_url=train_data_url\n",
    "    )\n",
    "    train_dataset.set_display_name(\"train-dataset\")\n",
    "    validate_train_dataset = validate_dataset(\n",
    "        train_dataset.output\n",
    "    )\n",
    "    validate_train_dataset.set_display_name(\"validate-train-dataset\")\n",
    "\n",
    "    eval_dataset = save_jsonl(\n",
    "        jsonl_url=eval_data_url\n",
    "    )\n",
    "    eval_dataset.set_display_name(\"eval-dataset\")\n",
    "    validate_eval_dataset = validate_dataset(\n",
    "        eval_dataset.output\n",
    "    )\n",
    "    validate_eval_dataset.set_display_name(\"validate-eval-dataset\")\n",
    "    \n",
    "    train_func = save_train_func(training_code_url=training_code_url)\n",
    "    \n",
    "    deepspeed_config = json_to_dict(\n",
    "        json_url=deepspeed_config_url,\n",
    "        extras_to_update=deepspeed_config_extras\n",
    "    )\n",
    "    deepspeed_config.set_display_name(\"deepspeed-config\")\n",
    "\n",
    "    job_id = generate_job_id()\n",
    "    job_id.set_caching_options(False)\n",
    "\n",
    "    job_script = generate_pytorchjob_script(\n",
    "        job_name=job_name,\n",
    "        job_id=job_id.output,\n",
    "        training_code=train_func.output,\n",
    "        deepspeed_config=deepspeed_config.output,\n",
    "        model_id=model_id,\n",
    "        train_dataset=validate_train_dataset.output,\n",
    "        eval_dataset=validate_eval_dataset.output,\n",
    "        mount_path=mount_path,\n",
    "        batch_size=batch_size,\n",
    "        num_train_epochs=num_train_epochs\n",
    "    )\n",
    "    \n",
    "    pytorchjob = run_pytorchjob(\n",
    "        job_name=job_name,\n",
    "        job_id=job_id.output,\n",
    "        base_image=base_image,\n",
    "        pvc_name=pvc.outputs['name'],\n",
    "        mount_path=mount_path,\n",
    "        exec_script=job_script.output,\n",
    "        master_replica=master_replica,\n",
    "        master_resources=master_resources,\n",
    "        worker_replica=worker_replica,\n",
    "        worker_resources=worker_resources,\n",
    "    )\n",
    "\n",
    "    isvc = run_isvc(\n",
    "        pvc_name=pvc.outputs['name'],\n",
    "        model_id=model_id,\n",
    "        pytorchjob_name=pytorchjob.output,\n",
    "    )\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcad247",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.compiler import Compiler\n",
    "\n",
    "Compiler().compile(fine_tuning_pipeline, package_path=\"luckyvicky-finetuning.yaml\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
