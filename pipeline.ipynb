{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b93328c9-e3b2-40d4-9162-5e5cf394b0ec",
   "metadata": {},
   "source": [
    "# Hello World pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dddda02b-e4e0-42e3-a5d8-53946f16bbbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "\n",
    "@dsl.component(base_image=\"python:3.11\")\n",
    "def hello_world(name:str) -> str:\n",
    "    return f\"hello world\\nnice to meet you {name}\"\n",
    "\n",
    "@dsl.component(base_image=\"python:3.11\")\n",
    "def print_str(hello_str:str):\n",
    "    print(hello_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee79faab-f198-42be-abf1-011c0745a16f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "\n",
    "@dsl.pipeline(name=\"hello-world\")\n",
    "def hello_world_pipeline(name:str):\n",
    "    hello_comp = hello_world(name=name)\n",
    "    print_comp = print_str(hello_str=hello_comp.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1850bba1-eb0f-4b85-b54e-41e9f10e87dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp.compiler import Compiler\n",
    "\n",
    "Compiler().compile(hello_world_pipeline, package_path=\"hello-world.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e996fdb1-044f-43e5-b649-35475c03683c",
   "metadata": {},
   "source": [
    "# Dataset processing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70a8460-67c8-4787-9039-a0a590d5711e",
   "metadata": {},
   "source": [
    "## install kfp-kubernetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afdad677-0341-4635-8325-b88a8c611234",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "pip install -q --no-cache-dir kfp[kubernetes]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42f5f3c-9289-4a22-b42d-babcb061619e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 데이터셋 다운로드 후 가공 컴포넌트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0635542d-8eb3-475a-a415-0b629379a56f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "from kfp.dsl import OutputPath\n",
    "\n",
    "@dsl.component(base_image=\"python:3.11\", packages_to_install=[\"datasets\"])\n",
    "def txt_to_qa_dataset(data_url: str, qa_dataset: OutputPath('Dataset')):    \n",
    "    txt_file = \"newjeans.txt\"\n",
    "    \n",
    "    import requests\n",
    "    \n",
    "    with requests.get(data_url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(txt_file, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "    \n",
    "    def gen_qa():\n",
    "        with open(txt_file, 'r') as f:\n",
    "            while True:\n",
    "                question = f.readline().replace('\\n', '')\n",
    "                answer = f.readline().replace('\\n', '')\n",
    "                yield {\"question\": question, \"answer\": answer}\n",
    "                buffer = f.readline()\n",
    "                if not buffer:\n",
    "                    break\n",
    "                    \n",
    "    from datasets import Dataset\n",
    "    newjeans_qa = Dataset.from_generator(gen_qa)\n",
    "    newjeans_qa.save_to_disk(qa_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9293057f-d9b9-4fee-9d3f-4d85cc6725da",
   "metadata": {},
   "source": [
    "## chat templete 형태로 데이터셋 가공 컴포넌트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40f9d156-5eb9-49ef-ad6f-ddeb22e3f57c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "from kfp.dsl import InputPath\n",
    "\n",
    "@dsl.component(base_image=\"python:3.11\", packages_to_install=[\"datasets\"])\n",
    "def process_chat_template(\n",
    "    system_prompt: str, \n",
    "    volume_mount: str, \n",
    "    qa_dataset: InputPath('Dataset')\n",
    ") -> str:\n",
    "    from datasets import load_from_disk\n",
    "    \n",
    "    newjeans_qa = load_from_disk(qa_dataset)\n",
    "    \n",
    "    def gen_chat_template(items):\n",
    "        messages = []\n",
    "        for q, a in zip(items['question'], items['answer']):\n",
    "            chat = []\n",
    "            # system role\n",
    "            chat.append({\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt\n",
    "            })\n",
    "            # user question\n",
    "            chat.append({\n",
    "                \"role\": \"user\",\n",
    "                \"content\": q\n",
    "            })\n",
    "            # assistant answer\n",
    "            chat.append({\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": a\n",
    "            })\n",
    "            messages.append(chat)\n",
    "        return {\"messages\": messages}\n",
    "    \n",
    "    prompt_template_qa = newjeans_qa.map(gen_chat_template, batched=True, remove_columns=newjeans_qa.features)\n",
    "    \n",
    "    import uuid\n",
    "    import os\n",
    "    \n",
    "    dataset_name = str(uuid.uuid4())\n",
    "    prompt_template_qa.save_to_disk(os.path.join(volume_mount, dataset_name))\n",
    "    return dataset_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1765504-e80e-4460-a995-a03da093cfb9",
   "metadata": {},
   "source": [
    "## pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8412b8ba-a228-4143-9cd0-784830e9b8e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "from kfp import kubernetes\n",
    "\n",
    "@dsl.pipeline(name=\"newjeans-fine-tuning\")\n",
    "def fine_tuning_pipeline(\n",
    "    data_url:str=\"https://docs.google.com/uc?export=download&id=1ycN8UktwSiMJ0cWwPXeLVIHJpBnUgEtE&confirm=t\",\n",
    "    system_prompt:str=\"당신은 K-pop 아이돌 그룹 뉴진스(NewJeans)의 정보를 알려주는 멋진 AI 어시스턴트입니다. 모든 대화는 한국어(Korean)로 합니다.\",\n",
    "):\n",
    "    pvc = kubernetes.CreatePVC(\n",
    "        # can also use pvc_name instead of pvc_name_suffix to use a pre-existing PVC\n",
    "        pvc_name='newjeans-finetuning-pvc-rwx',\n",
    "        access_modes=['ReadWriteMany'],\n",
    "        size='1024Gi',\n",
    "        storage_class_name='filestore-rwx',\n",
    "    )\n",
    "    pvc.set_caching_options(True)\n",
    "    \n",
    "    mount_path = '/data'\n",
    "    \n",
    "    dataset = txt_to_qa_dataset(data_url=data_url)\n",
    "    \n",
    "    chat_template_dataset = process_chat_template(\n",
    "        system_prompt=system_prompt, \n",
    "        volume_mount=mount_path, \n",
    "        qa_dataset=dataset.outputs[\"qa_dataset\"],\n",
    "    )\n",
    "    kubernetes.mount_pvc(chat_template_dataset, pvc_name=pvc.outputs['name'], mount_path=mount_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33eccec0-621a-4894-ae29-5986a00bfd7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp.compiler import Compiler\n",
    "\n",
    "Compiler().compile(fine_tuning_pipeline, package_path=\"newjeans-fine-tuning.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1043bd7c-8b54-4c4d-9c46-a718d4f4d1ee",
   "metadata": {},
   "source": [
    "# Training pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cdf445-e717-4610-b0c1-d85ac6463328",
   "metadata": {},
   "source": [
    "## training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bf3f163e-2da8-4916-83b2-cb8c5850206f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "\n",
    "@dsl.component(base_image=\"python:3.11\")\n",
    "def save_train_func(volume_mount:str) -> str:\n",
    "    def train(parameters):\n",
    "        import os\n",
    "        import json\n",
    "        from transformers import (\n",
    "            TrainingArguments, \n",
    "            logging,\n",
    "            AutoTokenizer, \n",
    "            AutoModelForCausalLM,\n",
    "            integrations,\n",
    "        )\n",
    "        from trl import SFTTrainer\n",
    "        from datasets import load_from_disk\n",
    "        from peft import LoraConfig, TaskType\n",
    "        import torch\n",
    "        import torch.distributed as dist\n",
    "        import mlflow\n",
    "        import evaluate\n",
    "        import numpy as np\n",
    "\n",
    "        ## NCCL log level\n",
    "        os.environ[\"NCCL_DEBUG\"] = 'INFO'\n",
    "\n",
    "        ## parse parameters\n",
    "        dirname = parameters[\"mount_path\"]\n",
    "        output_model = \"-\".join([parameters[\"output_model\"], parameters[\"random_suffix\"]])\n",
    "        mlflow_experiment_name = parameters[\"output_model\"]\n",
    "        mlflow_run_name = parameters[\"random_suffix\"]\n",
    "        batch_size = parameters[\"batch_size\"]\n",
    "        num_train_epochs = parameters[\"num_train_epochs\"]\n",
    "        dataset_name = parameters[\"dataset_name\"]\n",
    "        pretrained_model_name = parameters[\"pretrained_model_name\"]\n",
    "        deepspeed_config_path = os.path.join(dirname, parameters[\"deepspeed_config_file\"])\n",
    "\n",
    "        ## deepspeed\n",
    "        with open(deepspeed_config_path) as f:\n",
    "            deepspeed_config = json.load(f)\n",
    "        deepspeed_config[\"tensorboard\"][\"output_path\"] = os.path.join(dirname, output_model, \"logs\")\n",
    "\n",
    "        ## training arguments\n",
    "        args = TrainingArguments(\n",
    "            output_dir=os.path.join(dirname, output_model),\n",
    "\n",
    "            # batch/gradient_accumulation/epoch\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            gradient_accumulation_steps=4,\n",
    "            num_train_epochs=num_train_epochs,\n",
    "\n",
    "            # logging\n",
    "            logging_steps=1,\n",
    "            log_level='info',\n",
    "            logging_dir=os.path.join(dirname, output_model, \"logs\"),\n",
    "            report_to=[\"tensorboard\", \"mlflow\"],\n",
    "\n",
    "            # learning rate, scheduler\n",
    "            warmup_ratio=0.1,\n",
    "            learning_rate=2e-4,\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "\n",
    "            # dtype\n",
    "            bf16=True,\n",
    "\n",
    "            push_to_hub=False,\n",
    "\n",
    "            deepspeed=deepspeed_config,\n",
    "\n",
    "            # gradient_checkpointing\n",
    "            gradient_checkpointing=True,\n",
    "            gradient_checkpointing_kwargs={'use_reentrant':False},\n",
    "\n",
    "            disable_tqdm=True,\n",
    "        )\n",
    "\n",
    "        ## load dataset, tokenizer, model\n",
    "        dataset = load_from_disk(os.path.join(dirname, dataset_name))\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            pretrained_model_name, \n",
    "            torch_dtype=torch.bfloat16,\n",
    "        )\n",
    "        model.config.use_cache=False # for gradient checkpointing\n",
    "        model.config.pretraining_tp = 1\n",
    "\n",
    "        ## peft\n",
    "        peft_config = LoraConfig(\n",
    "            task_type=TaskType.CAUSAL_LM,\n",
    "            inference_mode=False,\n",
    "            r=16,\n",
    "            lora_alpha=32,\n",
    "            lora_dropout=0.05,\n",
    "            bias=\"none\",\n",
    "            use_rslora=True,\n",
    "        )\n",
    "\n",
    "        # metric\n",
    "        metric = evaluate.load('accuracy')\n",
    "\n",
    "        def compute_metrics(eval_pred):\n",
    "            predictions, labels = eval_pred\n",
    "            predictions = np.argmax(predictions, axis=1)\n",
    "            acc = metric.compute(predictions=predictions, references=labels)\n",
    "            return {{'accuracy': acc}}\n",
    "\n",
    "        ## trainer\n",
    "        trainer = SFTTrainer(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            args=args,\n",
    "            train_dataset=dataset,\n",
    "            peft_config=peft_config,\n",
    "            packing=True,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "\n",
    "        ## mlflow\n",
    "        os.environ[\"MLFLOW_EXPERIMENT_NAME\"] = mlflow_experiment_name\n",
    "        os.environ[\"MLFLOW_FLATTEN_PARAMS\"] = \"1\"\n",
    "        os.environ[\"MLFLOW_TRACKING_URI\"]=\"http://mlflow.mlflow\"\n",
    "        os.environ[\"MLFLOW_S3_ENDPOINT_URL\"]=\"http://minio-service.kubeflow:9000\"\n",
    "        os.environ[\"MLFLOW_S3_IGNORE_TLS\"]=\"true\"\n",
    "\n",
    "        # create run in master\n",
    "        if dist.get_rank() == 0:\n",
    "            mlflow.set_tracking_uri(os.environ[\"MLFLOW_TRACKING_URI\"])\n",
    "            mlflow.set_experiment(mlflow_experiment_name)\n",
    "            mlflow.start_run(run_name=mlflow_run_name)\n",
    "            dataset_for_log = mlflow.data.huggingface_dataset.from_huggingface(dataset)\n",
    "            mlflow.log_input(dataset_for_log, context=\"training\")\n",
    "\n",
    "        ## train and save model\n",
    "        trainer.model.print_trainable_parameters()\n",
    "        trainer.train()\n",
    "        trainer.save_model(os.path.join(dirname, output_model))\n",
    "\n",
    "        ## mlflow log model and end run\n",
    "        if dist.get_rank() == 0:\n",
    "            mlflow.transformers.log_model(\n",
    "                transformers_model={\n",
    "                    \"model\": trainer.model,\n",
    "                    \"tokenizer\": tokenizer,\n",
    "                },\n",
    "                artifact_path=\"model\",\n",
    "                task=\"text-generation\"\n",
    "            )\n",
    "            mlflow.end_run()\n",
    "    \n",
    "    import inspect\n",
    "    import textwrap\n",
    "    \n",
    "    func_code = inspect.getsource(train)\n",
    "    func_code = textwrap.dedent(func_code)\n",
    "    \n",
    "    import os\n",
    "    train_code_file = \"train.py\"\n",
    "    with open(os.path.join(volume_mount, train_code_file), \"w\") as f:\n",
    "        f.write(func_code)\n",
    "    return train_code_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7371d68-a07a-401d-b9cd-554f5c9469d4",
   "metadata": {},
   "source": [
    "## deepspeed config 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9fbeec4d-f8fe-4952-8d87-f265873035ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "\n",
    "@dsl.component(base_image=\"python:3.11\")\n",
    "def save_deepspeed_config(volume_mount:str) -> str:   \n",
    "    deepspeed_config = {\n",
    "        \"bf16\": {\n",
    "            \"enabled\": True\n",
    "        },\n",
    "        \"optimizer\": {\n",
    "            \"type\": \"AdamW\",\n",
    "            \"params\": {\n",
    "                \"lr\": \"auto\",\n",
    "                \"betas\": \"auto\",\n",
    "                \"eps\": \"auto\",\n",
    "                \"weight_decay\": \"auto\"\n",
    "            }\n",
    "        },\n",
    "\n",
    "        \"scheduler\": {\n",
    "            \"type\": \"WarmupCosineLR\",\n",
    "            \"params\": {\n",
    "                \"total_num_steps\": \"auto\",\n",
    "                \"warmup_min_ratio\": 0.1,\n",
    "                \"warmup_num_steps\": \"auto\",\n",
    "                \"cos_min_ratio\": 0.0001,\n",
    "            }\n",
    "        },\n",
    "        \"zero_optimization\": {\n",
    "            \"stage\": 3,\n",
    "            \"offload_optimizer\": {\n",
    "                \"device\": \"cpu\",\n",
    "                \"pin_memory\": True\n",
    "            },\n",
    "            \"offload_param\": {\n",
    "                \"device\": \"cpu\",\n",
    "                \"pin_memory\": True\n",
    "            },\n",
    "            \"overlap_comm\": True,\n",
    "            \"contiguous_gradients\": True,\n",
    "            \"sub_group_size\": 1e9,\n",
    "            \"reduce_bucket_size\": \"auto\",\n",
    "            \"stage3_prefetch_bucket_size\": \"auto\",\n",
    "            \"stage3_param_persistence_threshold\": \"auto\",\n",
    "            \"stage3_max_live_parameters\": 1e9,\n",
    "            \"stage3_max_reuse_distance\": 1e9,\n",
    "            \"stage3_gather_16bit_weights_on_model_save\": True\n",
    "        },\n",
    "        \"quantize_training\": {\n",
    "            \"enabled\": True,\n",
    "            \"quantize_verbose\": True,\n",
    "            \"quantizer_kernel\": True,\n",
    "            \"quantize-algo\": {\n",
    "                \"q_type\": \"symmetric\"\n",
    "            },\n",
    "            \"quantize_bits\": {\n",
    "                \"start_bits\": 16,\n",
    "                \"target_bits\": 8\n",
    "            },\n",
    "            \"quantize_schedule\": {\n",
    "                \"quantize_period\": 400,\n",
    "                \"schedule_offset\": 0\n",
    "            },\n",
    "            \"quantize_groups\": 8,\n",
    "        },\n",
    "        \"flops_profiler\": {\n",
    "            \"enabled\": True,\n",
    "            \"profile_step\": 1,\n",
    "            \"module_depth\": -1,\n",
    "            \"top_modules\": 1,\n",
    "            \"detailed\": False,\n",
    "        },\n",
    "        \"comms_logger\": {\n",
    "            \"enabled\": True,\n",
    "            \"verbose\": False,\n",
    "            \"prof_all\": True,\n",
    "            \"debug\": False\n",
    "        },\n",
    "        \"tensorboard\": {\n",
    "            \"enabled\": True,\n",
    "            # \"output_path\": os.path.join(dirname, model_name, \"logs\"),\n",
    "        },\n",
    "        \"gradient_accumulation_steps\": \"auto\",\n",
    "        \"gradient_clipping\": \"auto\",\n",
    "        \"steps_per_print\": \"auto\",\n",
    "        \"train_batch_size\": \"auto\",\n",
    "        \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "        \"wall_clock_breakdown\": False\n",
    "    }\n",
    "    import json\n",
    "    import os\n",
    "    deepspeed_config_file = \"deepspeed_config.json\"\n",
    "    with open(os.path.join(volume_mount, deepspeed_config_file), 'w') as f:\n",
    "        json.dump(deepspeed_config, f)\n",
    "        \n",
    "    return deepspeed_config_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34963a49-2f6f-4825-970e-8cbfab37698a",
   "metadata": {},
   "source": [
    "## PytorchJob 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442cd205-0fd9-4290-a0d1-3c8f1414bfde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "\n",
    "# parameters= {\n",
    "#     \"mount_path\": \"/data\",\n",
    "#     \"deepspeed_config_file\": \"deepspeed_config.json\",\n",
    "#     \"random_suffix\": \"\",\n",
    "#     \"output_model\": \"\",\n",
    "#     \"batch_size\": 4,\n",
    "#     \"num_train_epochs\": 40,\n",
    "#     \"dataset_name\": \"\",\n",
    "#     \"pretrained_model_name\": \"\",\n",
    "# }\n",
    "\n",
    "@dsl.component(base_image=\"python:3.11\", packages_to_install=[\"kubeflow-training\"])\n",
    "def run_pytorchjob(\n",
    "    base_image:str,\n",
    "    pvc_name:str,\n",
    "    train_func:str,\n",
    "    pretrained_model_name:str,\n",
    "    dataset_name:str,\n",
    "    output_model:str,\n",
    "    deepspeed_config_file:str,\n",
    "    batch_size:int,\n",
    "    num_train_epochs:int,\n",
    "    master_replica:int,\n",
    "    master_cpu:str,\n",
    "    master_memory:str,\n",
    "    master_gpu:int,\n",
    "    worker_replica:int,\n",
    "    worker_cpu:str,\n",
    "    worker_memory:str,\n",
    "    worker_gpu:int,\n",
    ") -> str:\n",
    "    ## populate parameters\n",
    "    parameters = {\n",
    "        \"mount_path\": \"/data\",\n",
    "        \"pretrained_model_name\": pretrained_model_name,\n",
    "        \"dataset_name\": dataset_name,\n",
    "        \"output_model\": output_model,\n",
    "        \"deepspeed_config_file\": deepspeed_config_file,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"num_train_epochs\": num_train_epochs,\n",
    "    }\n",
    "    \n",
    "    import string\n",
    "    import random\n",
    "     \n",
    "    letters_set = string.ascii_lowercase + string.digits\n",
    "    random_list = random.sample(letters_set,5)\n",
    "    uid = ''.join(random_list)\n",
    "     \n",
    "    parameters[\"random_suffix\"] = uid\n",
    "    \n",
    "    ## populate exec script\n",
    "    import os\n",
    "    import textwrap\n",
    "    \n",
    "    with open(os.path.join(parameters[\"mount_path\"], train_func), \"r\") as f:\n",
    "        func_code = f.read()\n",
    "    func_code = textwrap.dedent(func_code)\n",
    "    func_code = f\"{func_code}\\ntrain({parameters})\\n\"\n",
    "    \n",
    "    exec_script = textwrap.dedent(\n",
    "        \"\"\"\n",
    "            program_path=$(mktemp -d)\n",
    "            read -r -d '' SCRIPT << EOM\\n\n",
    "            {func_code}\n",
    "            EOM\n",
    "            printf \"%s\" \"$SCRIPT\" > $program_path/ephemeral_script.py\n",
    "            torchrun --nproc-per-node {gpu_cnt} $program_path/ephemeral_script.py\"\"\"\n",
    "    )\n",
    "\n",
    "    master_exec_script = exec_script.format(func_code=func_code, gpu_cnt=master_gpu)\n",
    "    worker_exec_script = exec_script.format(func_code=func_code, gpu_cnt=worker_gpu)\n",
    "    \n",
    "    ## declare pod spec\n",
    "    from kubernetes import client\n",
    "    from kubeflow.training.constants import constants\n",
    "    pod_template_spec = client.V1PodTemplateSpec(\n",
    "        metadata=client.V1ObjectMeta(annotations={constants.ISTIO_SIDECAR_INJECTION: \"false\"}), ## istio sidecar disable\n",
    "        spec=client.V1PodSpec(\n",
    "            restart_policy=\"Never\",\n",
    "            containers=[\n",
    "                client.V1Container(\n",
    "                    name=constants.PYTORCHJOB_CONTAINER,\n",
    "                    image=base_image,\n",
    "                    command=[\"bash\", \"-c\"],\n",
    "                    args=[master_exec_script],\n",
    "                    resources=client.V1ResourceRequirements(\n",
    "                        limits={\n",
    "                            \"cpu\": master_cpu,\n",
    "                            \"memory\": master_memory,\n",
    "                            \"nvidia.com/gpu\": master_gpu\n",
    "                        },\n",
    "                        requests={\n",
    "                            \"cpu\": master_cpu,\n",
    "                            \"memory\": master_memory,\n",
    "                            \"nvidia.com/gpu\": master_gpu\n",
    "                        }\n",
    "                    ),\n",
    "                    env=[ ## for mlflow artifact store\n",
    "                        client.V1EnvVar(\n",
    "                            name=\"AWS_ACCESS_KEY_ID\",\n",
    "                            value_from=client.V1EnvVarSource(\n",
    "                                secret_key_ref=client.V1SecretKeySelector(\n",
    "                                    name=\"mlpipeline-minio-artifact\",\n",
    "                                    key=\"accesskey\",\n",
    "                                )\n",
    "                            )\n",
    "                        ),\n",
    "                        client.V1EnvVar(\n",
    "                            name=\"AWS_SECRET_ACCESS_KEY\",\n",
    "                            value_from=client.V1EnvVarSource(\n",
    "                                secret_key_ref=client.V1SecretKeySelector(\n",
    "                                    name=\"mlpipeline-minio-artifact\",\n",
    "                                    key=\"secretkey\",\n",
    "                                )\n",
    "                            )\n",
    "                        ),\n",
    "                    ],\n",
    "                    volume_mounts=[\n",
    "                        client.V1VolumeMount(\n",
    "                            mount_path=parameters[\"mount_path\"],\n",
    "                            name=pvc_name,\n",
    "                            read_only=False,\n",
    "\n",
    "                        ),\n",
    "                        client.V1VolumeMount(\n",
    "                            mount_path=\"/dev/shm\",\n",
    "                            name=\"dshm\",\n",
    "                            read_only=False,\n",
    "                        ),\n",
    "                    ]\n",
    "                )\n",
    "            ],\n",
    "            volumes=[\n",
    "                client.V1Volume(\n",
    "                    name=pvc_name,\n",
    "                    persistent_volume_claim=client.V1PersistentVolumeClaimVolumeSource(\n",
    "                        claim_name=pvc_name,\n",
    "                        read_only=False,\n",
    "                    )\n",
    "                ),\n",
    "                client.V1Volume(\n",
    "                    name=\"dshm\",\n",
    "                    empty_dir=client.V1EmptyDirVolumeSource(\n",
    "                        medium=\"Memory\",\n",
    "                        size_limit=\"0.5Gi\"\n",
    "                    )\n",
    "                )\n",
    "            ]\n",
    "        )        \n",
    "    )\n",
    "    \n",
    "    ## get namespace\n",
    "    with open(\"/var/run/secrets/kubernetes.io/serviceaccount/namespace\", \"r\") as f:\n",
    "        namespace = f.read()\n",
    "    \n",
    "    ## declare pytorchjob\n",
    "    from kubeflow.training import models\n",
    "    \n",
    "    pytorchjob_name = \"-\".join([parameters[\"output_model\"], parameters[\"random_suffix\"]])\n",
    "    pytorchjob = models.KubeflowOrgV1PyTorchJob(\n",
    "        api_version=f\"{constants.KUBEFLOW_GROUP}/{constants.OPERATOR_VERSION}\",\n",
    "        kind=constants.PYTORCHJOB_KIND,\n",
    "        metadata=client.V1ObjectMeta(name=pytorchjob_name, namespace=namespace),\n",
    "        spec=models.KubeflowOrgV1PyTorchJobSpec(\n",
    "            run_policy=models.KubeflowOrgV1RunPolicy(clean_pod_policy=None),\n",
    "            pytorch_replica_specs={}\n",
    "        )\n",
    "    )\n",
    "    pytorchjob.spec.pytorch_replica_specs[constants.REPLICA_TYPE_MASTER] = models.KubeflowOrgV1ReplicaSpec(replicas=master_replica, template=pod_template_spec)\n",
    "    \n",
    "    import copy\n",
    "    worker_pod_template_spec = copy.deepcopy(pod_template_spec)\n",
    "    worker_pod_template_spec.spec.containers[0].args = [worker_exec_script]\n",
    "    worker_pod_template_spec.spec.containers[0].resources = client.V1ResourceRequirements(\n",
    "        limits={\"cpu\": worker_cpu, \"memory\": worker_memory, \"nvidia.com/gpu\": worker_gpu},\n",
    "        requests={\"cpu\": worker_cpu, \"memory\": worker_memory, \"nvidia.com/gpu\": worker_gpu},\n",
    "    )\n",
    "    pytorchjob.spec.pytorch_replica_specs[constants.REPLICA_TYPE_WORKER] = models.KubeflowOrgV1ReplicaSpec(replicas=worker_replica, template=worker_pod_template_spec)\n",
    "        \n",
    "    \n",
    "    ## create pytorchjob\n",
    "    from kubeflow.training import TrainingClient\n",
    "    training_client = TrainingClient()\n",
    "    training_client.create_pytorchjob(pytorchjob, namespace)\n",
    "    \n",
    "    ## wait till Running state\n",
    "    running_pytorchjob = training_client.wait_for_job_conditions(\n",
    "        name=pytorchjob.metadata.name,\n",
    "        namespace=pytorchjob.metadata.namespace,\n",
    "        job_kind=constants.PYTORCHJOB_KIND,\n",
    "        expected_conditions={constants.JOB_CONDITION_RUNNING}\n",
    "    )\n",
    "    \n",
    "    ## log master pod\n",
    "    training_client.get_job_logs(\n",
    "        name=running_pytorchjob.metadata.name,\n",
    "        namespace= running_pytorchjob.metadata.namespace,\n",
    "        container=constants.PYTORCHJOB_CONTAINER,\n",
    "        follow=True\n",
    "    )\n",
    "    \n",
    "    ## check job is succeeded\n",
    "    training_client.wait_for_job_conditions(\n",
    "        name=running_pytorchjob.metadata.name,\n",
    "        namespace=running_pytorchjob.metadata.namespace,\n",
    "        job_kind=constants.PYTORCHJOB_KIND,\n",
    "        expected_conditions={constants.JOB_CONDITION_SUCCEEDED}\n",
    "    )\n",
    "    return pytorchjob_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00328dbe-3d7c-4526-bd4a-83b1ec531d11",
   "metadata": {},
   "source": [
    "## pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "770cd28e-3e42-4031-832e-29dbca1fa31d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "from kfp import kubernetes\n",
    "\n",
    "@dsl.pipeline(name=\"newjeans-fine-tuning\")\n",
    "def fine_tuning_pipeline(\n",
    "    data_url:str=\"https://docs.google.com/uc?export=download&id=1ycN8UktwSiMJ0cWwPXeLVIHJpBnUgEtE&confirm=t\",\n",
    "    system_prompt:str=\"당신은 K-pop 아이돌 그룹 뉴진스(NewJeans)의 정보를 알려주는 멋진 AI 어시스턴트입니다. 모든 대화는 한국어(Korean)로 합니다.\",\n",
    "    base_image:str=\"miroirs/transformers-pytorch-deepspeed-latest-gpu:deepspeed-0.14.0-all\",\n",
    "    model_id:str=\"unsloth/llama-3-8b-Instruct\",\n",
    "    output_model_prefix:str=\"newjeans-finetuning\",\n",
    "    batch_size:int=4,\n",
    "    num_train_epochs:int=40,\n",
    "    master_replica:int=1,\n",
    "    master_cpu:str=\"2\",\n",
    "    master_memory:str=\"55Gi\",\n",
    "    master_gpu:int=1,\n",
    "    worker_replica:int=1,\n",
    "    worker_cpu:str=\"2\",\n",
    "    worker_memory:str=\"20Gi\",\n",
    "    worker_gpu:int=1,\n",
    "):\n",
    "    pvc = kubernetes.CreatePVC(\n",
    "        # can also use pvc_name instead of pvc_name_suffix to use a pre-existing PVC\n",
    "        pvc_name='newjeans-finetuning-pvc-rwx',\n",
    "        access_modes=['ReadWriteMany'],\n",
    "        size='1024Gi',\n",
    "        storage_class_name='filestore-rwx',\n",
    "    )\n",
    "    pvc.set_caching_options(True)\n",
    "    \n",
    "    mount_path = '/data'\n",
    "    \n",
    "    dataset = txt_to_qa_dataset(data_url=data_url)\n",
    "    \n",
    "    chat_template_dataset = process_chat_template(\n",
    "        system_prompt=system_prompt, \n",
    "        volume_mount=mount_path, \n",
    "        qa_dataset=dataset.outputs[\"qa_dataset\"],\n",
    "    )\n",
    "    kubernetes.mount_pvc(chat_template_dataset, pvc_name=pvc.outputs['name'], mount_path=mount_path)\n",
    "    \n",
    "    train_func = save_train_func(volume_mount=mount_path)\n",
    "    kubernetes.mount_pvc(train_func, pvc_name=pvc.outputs['name'], mount_path=mount_path)\n",
    "    \n",
    "    deepspeed_config = save_deepspeed_config(volume_mount=mount_path)\n",
    "    kubernetes.mount_pvc(deepspeed_config, pvc_name=pvc.outputs['name'], mount_path=mount_path)\n",
    "    \n",
    "    pytorchjob = run_pytorchjob(\n",
    "        base_image=base_image,\n",
    "        pvc_name=pvc.outputs['name'],\n",
    "        train_func=train_func.output,\n",
    "        pretrained_model_name=model_id,\n",
    "        dataset_name=chat_template_dataset.output,\n",
    "        output_model=output_model_prefix,\n",
    "        deepspeed_config_file=deepspeed_config.output,\n",
    "        batch_size=batch_size,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        master_replica=master_replica,\n",
    "        master_cpu=master_cpu,\n",
    "        master_memory=master_memory,\n",
    "        master_gpu=master_gpu,\n",
    "        worker_replica=worker_replica,\n",
    "        worker_cpu=worker_cpu,\n",
    "        worker_memory=worker_memory,\n",
    "        worker_gpu=worker_gpu,\n",
    "    )\n",
    "    kubernetes.mount_pvc(pytorchjob, pvc_name=pvc.outputs['name'], mount_path=mount_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2a1c50bd-0cd0-4b8d-ae68-ba6182da367d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp.compiler import Compiler\n",
    "\n",
    "Compiler().compile(fine_tuning_pipeline, package_path=\"newjeans-fine-tuning.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ad7c1c",
   "metadata": {},
   "source": [
    "# Serving pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5960b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "\n",
    "@dsl.component(base_image=\"python:3.11\", packages_to_install=[\"kserve\"])\n",
    "def run_isvc(\n",
    "    pvc_name: str,\n",
    "    pytorchjob_name: str,\n",
    "):\n",
    "    from kserve import constants\n",
    "    from kserve import (\n",
    "        V1beta1PredictorSpec,\n",
    "        V1beta1InferenceServiceSpec,\n",
    "        V1beta1InferenceService,\n",
    "    )\n",
    "    from kubernetes import client\n",
    "\n",
    "    ## get namespace\n",
    "    with open(\"/var/run/secrets/kubernetes.io/serviceaccount/namespace\", \"r\") as f:\n",
    "        namespace = f.read()\n",
    "    predictor_spec = V1beta1PredictorSpec(\n",
    "        containers=[\n",
    "            client.V1Container(\n",
    "                args=[\n",
    "                    \"--model-id\",\n",
    "                    f\"/mnt/models/{pytorchjob_name}\",\n",
    "                    \"--quantize\",\n",
    "                    \"bitsandbytes-nf4\",\n",
    "                ],\n",
    "                image=\"ghcr.io/huggingface/text-generation-inference:2.0\",\n",
    "                name=\"kserve-container\",\n",
    "                ports=[\n",
    "                    client.V1ContainerPort(\n",
    "                        container_port=8080,\n",
    "                        protocol=\"TCP\",\n",
    "                    )\n",
    "                ],\n",
    "                resources=client.V1ResourceRequirements(\n",
    "                    limits={\"cpu\": \"2\", \"memory\": \"40Gi\", \"nvidia.com/gpu\": \"1\"},\n",
    "                    requests={\"cpu\": \"2\", \"memory\": \"25Gi\", \"nvidia.com/gpu\": \"1\"},\n",
    "                ),\n",
    "                volume_mounts=[\n",
    "                    client.V1VolumeMount(\n",
    "                        mount_path=\"/mnt/models\",\n",
    "                        name=\"models\",\n",
    "                        read_only=False,\n",
    "                    ),\n",
    "                    client.V1VolumeMount(\n",
    "                        mount_path=\"/dev/shm\",\n",
    "                        name=\"dshm\",\n",
    "                        read_only=False,\n",
    "                    ),\n",
    "                ],\n",
    "            )\n",
    "        ],\n",
    "        max_replicas=1,\n",
    "        min_replicas=1,\n",
    "        volumes=[\n",
    "            client.V1Volume(\n",
    "                name=\"models\",\n",
    "                persistent_volume_claim=client.V1PersistentVolumeClaimVolumeSource(\n",
    "                    claim_name=pvc_name,\n",
    "                    read_only=False,\n",
    "                ),\n",
    "            ),\n",
    "            client.V1Volume(\n",
    "                name=\"dshm\",\n",
    "                empty_dir=client.V1EmptyDirVolumeSource(\n",
    "                    medium=\"Memory\", size_limit=\"0.5Gi\"\n",
    "                ),\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    inference_service_spec = V1beta1InferenceServiceSpec(predictor=predictor_spec)\n",
    "    inference_service = V1beta1InferenceService(\n",
    "        api_version=constants.KSERVE_V1BETA1,\n",
    "        kind=constants.KSERVE_KIND,\n",
    "        metadata=client.V1ObjectMeta(\n",
    "            name=\"newjeans\", \n",
    "            namespace=namespace, \n",
    "            annotations={\n",
    "                \"sidecar.istio.io/inject\": \"false\",\n",
    "                \"serving.kserve.io/enable-prometheus-scraping\": \"true\"\n",
    "            }),\n",
    "        spec=inference_service_spec,\n",
    "    )\n",
    "\n",
    "    from kserve import KServeClient\n",
    "    kserve_client = KServeClient()\n",
    "    kserve_client.create(inference_service, namespace=namespace, watch=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa1251f",
   "metadata": {},
   "source": [
    "## pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e979b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "from kfp import kubernetes\n",
    "\n",
    "@dsl.pipeline(name=\"newjeans-fine-tuning\")\n",
    "def fine_tuning_pipeline(\n",
    "    data_url:str=\"https://docs.google.com/uc?export=download&id=1ycN8UktwSiMJ0cWwPXeLVIHJpBnUgEtE&confirm=t\",\n",
    "    system_prompt:str=\"당신은 K-pop 아이돌 그룹 뉴진스(NewJeans)의 정보를 알려주는 멋진 AI 어시스턴트입니다. 모든 대화는 한국어(Korean)로 합니다.\",\n",
    "    base_image:str=\"miroirs/transformers-pytorch-deepspeed-latest-gpu:deepspeed-0.14.0-all\",\n",
    "    model_id:str=\"unsloth/llama-3-8b-Instruct\",\n",
    "    output_model_prefix:str=\"newjeans-finetuning\",\n",
    "    batch_size:int=4,\n",
    "    num_train_epochs:int=40,\n",
    "    master_replica:int=1,\n",
    "    master_cpu:str=\"2\",\n",
    "    master_memory:str=\"55Gi\",\n",
    "    master_gpu:int=1,\n",
    "    worker_replica:int=1,\n",
    "    worker_cpu:str=\"2\",\n",
    "    worker_memory:str=\"20Gi\",\n",
    "    worker_gpu:int=1,\n",
    "):\n",
    "    pvc = kubernetes.CreatePVC(\n",
    "        # can also use pvc_name instead of pvc_name_suffix to use a pre-existing PVC\n",
    "        pvc_name='newjeans-finetuning-pvc-rwx',\n",
    "        access_modes=['ReadWriteMany'],\n",
    "        size='1024Gi',\n",
    "        storage_class_name='filestore-rwx',\n",
    "    )\n",
    "    pvc.set_caching_options(True)\n",
    "    \n",
    "    mount_path = '/data'\n",
    "    \n",
    "    dataset = txt_to_qa_dataset(data_url=data_url)\n",
    "    \n",
    "    chat_template_dataset = process_chat_template(\n",
    "        system_prompt=system_prompt, \n",
    "        volume_mount=mount_path, \n",
    "        qa_dataset=dataset.outputs[\"qa_dataset\"],\n",
    "    )\n",
    "    kubernetes.mount_pvc(chat_template_dataset, pvc_name=pvc.outputs['name'], mount_path=mount_path)\n",
    "    \n",
    "    train_func = save_train_func(volume_mount=mount_path)\n",
    "    kubernetes.mount_pvc(train_func, pvc_name=pvc.outputs['name'], mount_path=mount_path)\n",
    "    \n",
    "    deepspeed_config = save_deepspeed_config(volume_mount=mount_path)\n",
    "    kubernetes.mount_pvc(deepspeed_config, pvc_name=pvc.outputs['name'], mount_path=mount_path)\n",
    "    \n",
    "    pytorchjob = run_pytorchjob(\n",
    "        base_image=base_image,\n",
    "        pvc_name=pvc.outputs['name'],\n",
    "        train_func=train_func.output,\n",
    "        pretrained_model_name=model_id,\n",
    "        dataset_name=chat_template_dataset.output,\n",
    "        output_model=output_model_prefix,\n",
    "        deepspeed_config_file=deepspeed_config.output,\n",
    "        batch_size=batch_size,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        master_replica=master_replica,\n",
    "        master_cpu=master_cpu,\n",
    "        master_memory=master_memory,\n",
    "        master_gpu=master_gpu,\n",
    "        worker_replica=worker_replica,\n",
    "        worker_cpu=worker_cpu,\n",
    "        worker_memory=worker_memory,\n",
    "        worker_gpu=worker_gpu,\n",
    "    )\n",
    "    kubernetes.mount_pvc(pytorchjob, pvc_name=pvc.outputs['name'], mount_path=mount_path)\n",
    "\n",
    "    isvc = run_isvc(\n",
    "        pvc_name=pvc.outputs[\"name\"],\n",
    "        pytorchjob_name=pytorchjob.output,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcad247",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.compiler import Compiler\n",
    "\n",
    "Compiler().compile(fine_tuning_pipeline, package_path=\"newjeans-fine-tuning.yaml\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
