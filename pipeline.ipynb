{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b93328c9-e3b2-40d4-9162-5e5cf394b0ec",
   "metadata": {},
   "source": [
    "# Hello World pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddda02b-e4e0-42e3-a5d8-53946f16bbbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "\n",
    "@dsl.component(base_image=\"python:3.11\")\n",
    "def hello_world(name:str) -> str:\n",
    "    return f\"hello world\\nnice to meet you {name}\"\n",
    "\n",
    "@dsl.component(base_image=\"python:3.11\")\n",
    "def print_str(hello_str:str):\n",
    "    print(hello_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee79faab-f198-42be-abf1-011c0745a16f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "\n",
    "@dsl.pipeline(name=\"hello-world\")\n",
    "def hello_world_pipeline(name:str):\n",
    "    hello_comp = hello_world(name=name)\n",
    "    print_comp = print_str(hello_str=hello_comp.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1850bba1-eb0f-4b85-b54e-41e9f10e87dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp.compiler import Compiler\n",
    "\n",
    "Compiler().compile(hello_world_pipeline, package_path=\"hello-world.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e996fdb1-044f-43e5-b649-35475c03683c",
   "metadata": {},
   "source": [
    "# Finetuning pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70a8460-67c8-4787-9039-a0a590d5711e",
   "metadata": {},
   "source": [
    "## install kfp-kubernetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdad677-0341-4635-8325-b88a8c611234",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -q --no-cache-dir 'kfp[kubernetes]'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42f5f3c-9289-4a22-b42d-babcb061619e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 데이터셋 다운로드, 검증 컴포넌트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0635542d-8eb3-475a-a415-0b629379a56f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "from kfp.dsl import Output, Dataset\n",
    "\n",
    "@dsl.component(base_image=\"python:3.11\", packages_to_install=[\"datasets\"])\n",
    "def save_jsonl(\n",
    "    jsonl_url: str,\n",
    "    output_dataset: Output[Dataset]\n",
    "):\n",
    "    import requests\n",
    "    from datasets import load_dataset\n",
    "\n",
    "    jsonl_file = \"temp.jsonl\"\n",
    "    # fetch jsonl file and save temp file\n",
    "    with requests.get(jsonl_url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(jsonl_file, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "\n",
    "    # load dataset from jsonl\n",
    "    dataset = load_dataset(\"json\", data_files=jsonl_file)\n",
    "    # save jsonl\n",
    "    dataset['train'].to_json(output_dataset.path, force_ascii=False)\n",
    "    output_dataset.metadata[\"original_url\"] = jsonl_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f32bff1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "from kfp.dsl import Input, Output, Dataset\n",
    "\n",
    "@dsl.component(base_image=\"python:3.11\", packages_to_install=[\"datasets\"])\n",
    "def validate_dataset(\n",
    "    jsonl_dataset: Input[Dataset],\n",
    "    valid_dataset: Output[Dataset]\n",
    "):\n",
    "    from datasets import load_dataset\n",
    "    \n",
    "    dataset = load_dataset(\"json\", data_files=jsonl_dataset.path)\n",
    "    \n",
    "    # conversational format validation\n",
    "    def is_chat_template(examples):\n",
    "        messages = examples[\"messages\"]\n",
    "        result = []\n",
    "        for message in messages:\n",
    "            checked = 0\n",
    "            for item in message:\n",
    "                if isinstance(item[\"role\"], str) and isinstance(item[\"content\"], str):\n",
    "                    checked += 1\n",
    "            result.append(len(message) == checked and checked > 0)\n",
    "        return result\n",
    "\n",
    "\n",
    "    feature_set = dataset['train'].features\n",
    "    if \"messages\" in feature_set:\n",
    "        dataset = dataset.filter(is_chat_template, batched=True).select_columns(\"messages\")\n",
    "    elif ({\"prompt\", \"completion\"}.issubset(feature_set) and \n",
    "          feature_set[\"prompt\"].dtype == 'string' and feature_set[\"completion\"].dtype == 'string'):\n",
    "        dataset = dataset.select_columns([\"prompt\", \"completion\"])\n",
    "    else:\n",
    "        raise ValueError(\"data should be conversational format or instruction format\") \n",
    "    dataset['train'].to_json(valid_dataset.path, force_ascii=False)\n",
    "    valid_dataset.metadata[\"original_url\"]= jsonl_dataset.metadata[\"original_url\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cdf445-e717-4610-b0c1-d85ac6463328",
   "metadata": {},
   "source": [
    "## training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf3f163e-2da8-4916-83b2-cb8c5850206f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "\n",
    "@dsl.component(base_image=\"python:3.11\", packages_to_install=[\"requests\"])\n",
    "def save_train_func(training_code_url:str) -> str:\n",
    "    import requests\n",
    "\n",
    "    response = requests.get(training_code_url)\n",
    "    training_code = response.content.decode('utf-8')\n",
    "\n",
    "    print(f\"training code:\\n\\n{training_code}\")\n",
    "\n",
    "    return training_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7371d68-a07a-401d-b9cd-554f5c9469d4",
   "metadata": {},
   "source": [
    "## deepspeed config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fbeec4d-f8fe-4952-8d87-f265873035ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "\n",
    "@dsl.component(base_image=\"python:3.11\", packages_to_install=[\"requests\"])\n",
    "def json_to_dict(\n",
    "    json_url:str,\n",
    "    extras_to_update:str,\n",
    ") -> dict:\n",
    "    import requests\n",
    "    import json\n",
    "\n",
    "    headers = {\"Content-type\": \"application/json\"}\n",
    "    response = requests.get(json_url, headers=headers)\n",
    "    json_data = response.json()\n",
    "\n",
    "    # add extras\n",
    "    def recursive_update(original, updates):\n",
    "        for key, value in updates.items():\n",
    "            # 둘다 dictionary면 재귀적으로 병합\n",
    "            if isinstance(value, dict) and key in original and isinstance(original[key], dict):\n",
    "                recursive_update(original[key], value)\n",
    "            # 업데이트\n",
    "            else:\n",
    "                original[key] = value\n",
    "    if extras_to_update:\n",
    "        extras_to_update = json.loads(extras_to_update)\n",
    "    \n",
    "    recursive_update(json_data, extras_to_update)\n",
    "\n",
    "    print(f\"json data:\\n\\n{json.dumps(json_data, indent=4)}\")\n",
    "\n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34963a49-2f6f-4825-970e-8cbfab37698a",
   "metadata": {},
   "source": [
    "## PytorchJob 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b46be1",
   "metadata": {},
   "source": [
    "### job 실행 스크립트 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf55f5e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "from kfp.dsl import Input, Dataset\n",
    "from typing import NamedTuple\n",
    "\n",
    "@dsl.component(base_image=\"python:3.11\")\n",
    "def generate_pytorchjob_script(\n",
    "    job_name:str,\n",
    "    training_code:str,\n",
    "    deepspeed_config:dict,\n",
    "    model_id:str,\n",
    "    train_dataset:Input[Dataset],\n",
    "    eval_dataset:Input[Dataset],\n",
    "    mount_path:str,\n",
    "    batch_size:int,\n",
    "    num_train_epochs:int,\n",
    ") -> NamedTuple('outputs', job_id=str, exec_script=str):\n",
    "    import os\n",
    "    import json\n",
    "    import string\n",
    "    import random\n",
    "\n",
    "    ## create job id\n",
    "    letters_set = string.ascii_lowercase + string.digits\n",
    "    random_list = random.sample(letters_set,5)\n",
    "    job_id = ''.join(random_list)\n",
    "\n",
    "    train_dataset_uri = train_dataset.uri\n",
    "    eval_dataset_uri = eval_dataset.uri\n",
    "    \n",
    "    deepspeed_config[\"tensorboard\"][\"output_path\"] = os.path.join(mount_path, f\"{job_name}-{job_id}/logs\")\n",
    "\n",
    "    exec_script = f\"\"\"\n",
    "program_path=$(mktemp -d)\n",
    "read -r -d '' SCRIPT << EOM\\n\n",
    "{training_code}\n",
    "EOM\n",
    "printf \"%s\" \"$SCRIPT\" > $program_path/ephemeral_script.py\n",
    "read -r -d '' SCRIPT << EOM\\n\n",
    "{json.dumps(deepspeed_config)}\n",
    "EOM\n",
    "printf \"%s\" \"$SCRIPT\" > $program_path/deepspeed.json\n",
    "torchrun --nnodes $WORLD_SIZE --nproc_per_node <<gpu_cnt>> --master-addr $MASTER_ADDR \\\n",
    "--master-port $MASTER_PORT --node-rank $RANK $program_path/ephemeral_script.py \\\n",
    "--model_path {model_id} --train_dataset_uri {train_dataset_uri} --eval_dataset_uri {eval_dataset_uri} \\\n",
    "--training_job_name {job_name} --training_job_id {job_id} --mount_path {mount_path} \\\n",
    "--per_device_train_batch_size {batch_size} --per_device_eval_batch_size {batch_size} \\\n",
    "--num_train_epochs {num_train_epochs} --deepspeed $program_path/deepspeed.json \\\n",
    "\"\"\"\n",
    "    print(f\"pytorchjob script:\\n\\n{exec_script}\")\n",
    "    print(f\"pytorchjob job_id: {job_id}\")\n",
    "    outputs = NamedTuple('outputs', job_id=str, exec_script=str)\n",
    "    return outputs(job_id, exec_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15c93af",
   "metadata": {},
   "source": [
    "### job 생성, 트레이싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "442cd205-0fd9-4290-a0d1-3c8f1414bfde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "\n",
    "@dsl.component(base_image=\"python:3.11\", packages_to_install=[\"kubeflow-training\"])\n",
    "def run_pytorchjob(\n",
    "    job_name:str,\n",
    "    job_id:str,\n",
    "    base_image:str,\n",
    "    pvc_name:str,\n",
    "    mount_path:str,\n",
    "    exec_script:str,\n",
    "    master_replica:int,\n",
    "    worker_replica:int,\n",
    "    requests_resources:str,\n",
    "    limits_resources:str,\n",
    ") -> str:\n",
    "    import json\n",
    "    from kubernetes import client\n",
    "    from kubeflow.training.constants import constants\n",
    "    from kubeflow.training import models\n",
    "\n",
    "    # parse node resources\n",
    "    requests_resources = json.loads(requests_resources)\n",
    "    limits_resources = json.loads(limits_resources)\n",
    "\n",
    "    # populate nproc_per_node argument\n",
    "    exec_script = exec_script.replace(\"<<gpu_cnt>>\", requests_resources[\"nvidia.com/gpu\"])\n",
    "    \n",
    "    ## declare training pod spec    \n",
    "    pod_template_spec = client.V1PodTemplateSpec(\n",
    "        metadata=client.V1ObjectMeta(annotations={constants.ISTIO_SIDECAR_INJECTION: \"false\"}), ## istio sidecar disable\n",
    "        spec=client.V1PodSpec(\n",
    "            restart_policy=\"Never\",\n",
    "            containers=[\n",
    "                client.V1Container(\n",
    "                    name=constants.PYTORCHJOB_CONTAINER,\n",
    "                    image=base_image,\n",
    "                    command=[\"bash\", \"-c\"],\n",
    "                    args=[exec_script],\n",
    "                    resources=client.V1ResourceRequirements(\n",
    "                        limits=limits_resources,\n",
    "                        requests=requests_resources\n",
    "                    ),\n",
    "                    env=[ ## for mlflow artifact store\n",
    "                        client.V1EnvVar(\n",
    "                            name=\"AWS_ACCESS_KEY_ID\",\n",
    "                            value_from=client.V1EnvVarSource(\n",
    "                                secret_key_ref=client.V1SecretKeySelector(\n",
    "                                    name=\"mlpipeline-minio-artifact\",\n",
    "                                    key=\"accesskey\",\n",
    "                                )\n",
    "                            )\n",
    "                        ),\n",
    "                        client.V1EnvVar(\n",
    "                            name=\"AWS_SECRET_ACCESS_KEY\",\n",
    "                            value_from=client.V1EnvVarSource(\n",
    "                                secret_key_ref=client.V1SecretKeySelector(\n",
    "                                    name=\"mlpipeline-minio-artifact\",\n",
    "                                    key=\"secretkey\",\n",
    "                                )\n",
    "                            )\n",
    "                        ),\n",
    "                    ],\n",
    "                    volume_mounts=[\n",
    "                        client.V1VolumeMount(\n",
    "                            mount_path=mount_path,\n",
    "                            name=pvc_name,\n",
    "                            read_only=False,\n",
    "\n",
    "                        ),\n",
    "                        client.V1VolumeMount(\n",
    "                            mount_path=\"/dev/shm\",\n",
    "                            name=\"dshm\",\n",
    "                            read_only=False,\n",
    "                        ),\n",
    "                    ]\n",
    "                )\n",
    "            ],\n",
    "            volumes=[\n",
    "                client.V1Volume(\n",
    "                    name=pvc_name,\n",
    "                    persistent_volume_claim=client.V1PersistentVolumeClaimVolumeSource(\n",
    "                        claim_name=pvc_name,\n",
    "                        read_only=False,\n",
    "                    )\n",
    "                ),\n",
    "                client.V1Volume(\n",
    "                    name=\"dshm\",\n",
    "                    empty_dir=client.V1EmptyDirVolumeSource(\n",
    "                        medium=\"Memory\",\n",
    "                        size_limit=\"1.0Gi\"\n",
    "                    )\n",
    "                )\n",
    "            ]\n",
    "        )        \n",
    "    )\n",
    "    \n",
    "    ## get namespace\n",
    "    with open(\"/var/run/secrets/kubernetes.io/serviceaccount/namespace\", \"r\") as f:\n",
    "        namespace = f.read()\n",
    "    \n",
    "    ## declare pytorchjob    \n",
    "    pytorchjob_name = f\"{job_name}-{job_id}\"\n",
    "    pytorchjob = models.KubeflowOrgV1PyTorchJob(\n",
    "        api_version=constants.API_VERSION,\n",
    "        kind=constants.PYTORCHJOB_KIND,\n",
    "        metadata=client.V1ObjectMeta(name=pytorchjob_name, namespace=namespace),\n",
    "        spec=models.KubeflowOrgV1PyTorchJobSpec(\n",
    "            run_policy=models.KubeflowOrgV1RunPolicy(clean_pod_policy=None),\n",
    "            pytorch_replica_specs={}\n",
    "        )\n",
    "    )\n",
    "    pytorchjob.spec.pytorch_replica_specs[constants.REPLICA_TYPE_MASTER] = models.KubeflowOrgV1ReplicaSpec(replicas=master_replica, template=pod_template_spec)\n",
    "    pytorchjob.spec.pytorch_replica_specs[constants.REPLICA_TYPE_WORKER] = models.KubeflowOrgV1ReplicaSpec(replicas=worker_replica, template=pod_template_spec)\n",
    "    \n",
    "    ## create pytorchjob\n",
    "    from kubeflow.training import TrainingClient\n",
    "    training_client = TrainingClient()\n",
    "    training_client.create_job(pytorchjob, namespace=namespace)\n",
    "    \n",
    "    ## wait till Running state\n",
    "    running_pytorchjob = training_client.wait_for_job_conditions(\n",
    "        name=pytorchjob.metadata.name,\n",
    "        namespace=pytorchjob.metadata.namespace,\n",
    "        job_kind=constants.PYTORCHJOB_KIND,\n",
    "        expected_conditions={constants.JOB_CONDITION_RUNNING}\n",
    "    )\n",
    "    \n",
    "    ## log master pod\n",
    "    training_client.get_job_logs(\n",
    "        name=running_pytorchjob.metadata.name,\n",
    "        namespace= running_pytorchjob.metadata.namespace,\n",
    "        job_kind=constants.PYTORCHJOB_KIND,\n",
    "        is_master=True,\n",
    "        follow=True\n",
    "    )\n",
    "    \n",
    "    ## check job is succeeded\n",
    "    training_client.wait_for_job_conditions(\n",
    "        name=running_pytorchjob.metadata.name,\n",
    "        namespace=running_pytorchjob.metadata.namespace,\n",
    "        job_kind=constants.PYTORCHJOB_KIND,\n",
    "        expected_conditions={constants.JOB_CONDITION_SUCCEEDED}\n",
    "    )\n",
    "\n",
    "    ## delete job\n",
    "    training_client.delete_job(\n",
    "        name=running_pytorchjob.metadata.name,\n",
    "        namespace=running_pytorchjob.metadata.namespace,\n",
    "        job_kind=constants.PYTORCHJOB_KIND,\n",
    "    )\n",
    "\n",
    "    return pytorchjob_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00328dbe-3d7c-4526-bd4a-83b1ec531d11",
   "metadata": {},
   "source": [
    "## pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "770cd28e-3e42-4031-832e-29dbca1fa31d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "from kfp import kubernetes\n",
    "\n",
    "@dsl.pipeline(name=\"luckyvicky-finetuning\")\n",
    "def finetuning_pipeline(\n",
    "    train_data_url:str=\"https://drive.google.com/uc?id=1YHU91AS53lIqoSF8NUXxZV-Rnd2vHH_6\",\n",
    "    eval_data_url:str=\"https://drive.google.com/uc?id=1loRPD53t6K0kdMRa6lv7qRO5rzDLanRR\",\n",
    "    training_code_url:str=\"https://github.com/questcollector/llmops-manifests/raw/refs/heads/main/script/sft-train.py\",\n",
    "    deepspeed_config_url:str=\"https://github.com/questcollector/llmops-manifests/raw/refs/heads/main/script/deepspeed.json\",\n",
    "    deepspeed_config_extras:str=\"{}\",\n",
    "    base_image:str=\"asia-northeast3-docker.pkg.dev/tokyo-data-452811-t2/llmops/transformers-deepspeed:v4.49.0-v0.16.4-24.06\",\n",
    "    model_id:str=\"unsloth/Meta-Llama-3.1-8B-Instruct\",\n",
    "    job_name:str=\"luckyvicky\",\n",
    "    batch_size:int=4,\n",
    "    num_train_epochs:int=10,\n",
    "    master_replica:int=1,\n",
    "    worker_replica:int=1,\n",
    "    requests_resources:str='{\"cpu\":\"2\", \"memory\": \"20Gi\", \"nvidia.com/gpu\": \"1\"}',\n",
    "    limits_resources:str='{\"cpu\":\"4\", \"memory\": \"55Gi\", \"nvidia.com/gpu\": \"1\"}',\n",
    "):\n",
    "    \"\"\"finetuning with huggingface TRL SFTTrainer\n",
    "\n",
    "    Args:\n",
    "        train_data_url: url of train dataset in jsonl file.\n",
    "        eval_data_url: url of evaluation dataset in jsonl file.\n",
    "        training_code_url: url of training code.\n",
    "        deepspeed_config_url: url of deepspeed config in json format.\n",
    "        deepspeed_config_extras: extra updates of key, value for deepspeed config in json format.\n",
    "        base_image: base image for pytorchjob.\n",
    "        model_id: pretrained model for finetuning.\n",
    "        job_name: prefix of pytorchjob/output model\n",
    "        batch_size: batch size as training argument.\n",
    "        num_train_epochs: number of training epochs as training argument.\n",
    "        master_replica: replica size for pytorchjob master nodes\n",
    "        worker_replica: replica size for pytorchjob worker nodes\n",
    "        requests_resources: training job resource requests in json format\n",
    "        limits_resources: training job resource limits in json format\n",
    "\n",
    "    Returns:\n",
    "        \n",
    "    \"\"\"\n",
    "    # create rwx PVC\n",
    "    pvc = kubernetes.CreatePVC(\n",
    "        # can also use pvc_name instead of pvc_name_suffix to use a pre-existing PVC\n",
    "        pvc_name='luckyvicky-finetuning-pvc-rwx',\n",
    "        access_modes=['ReadWriteMany'],\n",
    "        size='1024Gi',\n",
    "        storage_class_name='filestore-rwx',\n",
    "    )\n",
    "    pvc.set_caching_options(True)\n",
    "    \n",
    "    mount_path = '/train'\n",
    "    \n",
    "    # get datasets\n",
    "    \n",
    "    train_dataset = save_jsonl(\n",
    "        jsonl_url=train_data_url\n",
    "    )\n",
    "    validated_train_dataset = validate_dataset(\n",
    "        jsonl_dataset=train_dataset.output\n",
    "    )\n",
    "\n",
    "    eval_dataset = save_jsonl(\n",
    "        jsonl_url=eval_data_url\n",
    "    )\n",
    "    validated_eval_dataset = validate_dataset(\n",
    "        jsonl_dataset=eval_dataset.output\n",
    "    )\n",
    "    \n",
    "    # fetch training code\n",
    "    train_func = save_train_func(training_code_url=training_code_url)\n",
    "    \n",
    "    # fetch deepspeed config\n",
    "    deepspeed = json_to_dict(\n",
    "        json_url=deepspeed_config_url,\n",
    "        extras_to_update=deepspeed_config_extras\n",
    "    )\n",
    "\n",
    "    # generate job script\n",
    "    job_id_and_script = generate_pytorchjob_script(\n",
    "        job_name=job_name,\n",
    "        training_code=train_func.output,\n",
    "        deepspeed_config=deepspeed.output,\n",
    "        model_id=model_id,\n",
    "        train_dataset=validated_train_dataset.output,\n",
    "        eval_dataset=validated_eval_dataset.output,\n",
    "        mount_path=mount_path,\n",
    "        batch_size=batch_size,\n",
    "        num_train_epochs=num_train_epochs\n",
    "    )\n",
    "    \n",
    "    # create pytorchjob and tracing status/logs\n",
    "    pytorchjob = run_pytorchjob(\n",
    "        job_name=job_name,\n",
    "        job_id=job_id_and_script.outputs['job_id'],\n",
    "        base_image=base_image,\n",
    "        pvc_name=pvc.outputs['name'],\n",
    "        mount_path=mount_path,\n",
    "        exec_script=job_id_and_script.outputs['exec_script'],\n",
    "        master_replica=master_replica,\n",
    "        worker_replica=worker_replica,\n",
    "        requests_resources=requests_resources,\n",
    "        limits_resources=limits_resources,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a1c50bd-0cd0-4b8d-ae68-ba6182da367d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp.compiler import Compiler\n",
    "\n",
    "Compiler().compile(finetuning_pipeline, package_path=\"luckyvicky-finetuning.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ad7c1c",
   "metadata": {},
   "source": [
    "# Serving pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e064a1d0",
   "metadata": {},
   "source": [
    "## InferenceService 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5960b01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "\n",
    "@dsl.component(base_image=\"python:3.11\", packages_to_install=[\"kserve\"])\n",
    "def run_isvc(\n",
    "    pvc_name: str,\n",
    "    model_id:str,\n",
    "    pytorchjob_name: str,\n",
    ") -> str:\n",
    "    import json\n",
    "    from kserve import constants\n",
    "    from kserve import (\n",
    "        V1beta1PredictorSpec,\n",
    "        V1beta1InferenceServiceSpec,\n",
    "        V1beta1InferenceService,\n",
    "    )\n",
    "    from kubernetes import client\n",
    "    job_name, job_id = pytorchjob_name.split(\"-\")\n",
    "\n",
    "    lora_modules = {\n",
    "        \"name\": job_name,\n",
    "        \"path\": f\"/mnt/models/{pytorchjob_name}\",\n",
    "        \"base_model_name\": model_id\n",
    "    }\n",
    "\n",
    "    ## get namespace\n",
    "    with open(\"/var/run/secrets/kubernetes.io/serviceaccount/namespace\", \"r\") as f:\n",
    "        namespace = f.read()\n",
    "    predictor_spec = V1beta1PredictorSpec(\n",
    "        containers=[\n",
    "            client.V1Container(\n",
    "                args=[\n",
    "                    \"--model\", f\"{model_id}\",\n",
    "                    \"--dtype\", \"bfloat16\",\n",
    "                    \"--max-model-len\", \"16384\",\n",
    "                    \"--quantization\", \"bitsandbytes\",\n",
    "                    \"--load-format\", \"bitsandbytes\",\n",
    "                    \"--enable-lora\",\n",
    "                    \"--lora-modules\", \n",
    "                    json.dumps(lora_modules),\n",
    "                    \"--otlp-traces-endpoint\",\n",
    "                    \"grpc://$(HOST_IP):4317\",\n",
    "                    \"--max-lora-rank\", \"32\",\n",
    "                    \"--enforce-eager\"\n",
    "                ],\n",
    "                image=\"asia-northeast3-docker.pkg.dev/tokyo-data-452811-t2/llmops/vllm/vllm-openai:v0.7.3-otel-bitsandbytes\",\n",
    "                name=\"kserve-container\",\n",
    "                ports=[ \n",
    "                    client.V1ContainerPort(\n",
    "                        container_port=8000,\n",
    "                        protocol=\"TCP\",\n",
    "                    )\n",
    "                ],\n",
    "                resources=client.V1ResourceRequirements(\n",
    "                    limits={\"cpu\": \"8\", \"memory\": \"40Gi\", \"nvidia.com/gpu\": \"1\"},\n",
    "                    requests={\"cpu\": \"4\", \"memory\": \"25Gi\", \"nvidia.com/gpu\": \"1\"},\n",
    "                ),\n",
    "                env=[\n",
    "                    client.V1EnvVar(\n",
    "                        name=\"STORAGE_URI\",\n",
    "                        value=f\"pvc://{pvc_name}\"\n",
    "                    ),\n",
    "                    client.V1EnvVar(\n",
    "                        name=\"OTEL_SERVICE_NAME\",\n",
    "                        value=f\"{model_id.split('/')[1]}\"\n",
    "                    ),\n",
    "                    client.V1EnvVar(\n",
    "                        name=\"OTEL_EXPORTER_OTLP_TRACES_INSECURE\",\n",
    "                        value=\"true\"\n",
    "                    ),\n",
    "                    client.V1EnvVar(\n",
    "                        name=\"HOST_IP\",\n",
    "                        value_from=client.V1EnvVarSource(\n",
    "                            field_ref=client.V1ObjectFieldSelector(\n",
    "                                field_path=\"status.hostIP\"\n",
    "                            )\n",
    "                        )\n",
    "                    ),\n",
    "                    client.V1EnvVar(\n",
    "                        name=\"OTEL_TRACES_EXPORTER\",\n",
    "                        value=\"otlp\"\n",
    "                    )\n",
    "                ],\n",
    "                volume_mounts=[\n",
    "                    client.V1VolumeMount(\n",
    "                        mount_path=\"/dev/shm\",\n",
    "                        name=\"dshm\",\n",
    "                        read_only=False,\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "        ],\n",
    "        max_replicas=1,\n",
    "        min_replicas=1,\n",
    "        volumes=[\n",
    "            client.V1Volume(\n",
    "                name=\"dshm\",\n",
    "                empty_dir=client.V1EmptyDirVolumeSource(\n",
    "                    medium=\"Memory\", size_limit=\"0.5Gi\"\n",
    "                ),\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    inference_service_spec = V1beta1InferenceServiceSpec(predictor=predictor_spec)\n",
    "    inference_service = V1beta1InferenceService(\n",
    "        api_version=constants.KSERVE_V1BETA1,\n",
    "        kind=constants.KSERVE_KIND,\n",
    "        metadata=client.V1ObjectMeta(\n",
    "            name=job_name, \n",
    "            namespace=namespace, \n",
    "            annotations={\n",
    "                \"sidecar.istio.io/inject\": \"false\",\n",
    "                \"serving.kserve.io/enable-prometheus-scraping\": \"true\"\n",
    "            }),\n",
    "        spec=inference_service_spec,\n",
    "    )\n",
    "\n",
    "    from kserve import KServeClient\n",
    "    kserve_client = KServeClient()\n",
    "    kserve_client.create(inference_service, namespace=namespace)\n",
    "    return job_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa1251f",
   "metadata": {},
   "source": [
    "## pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21798418",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "from kfp import kubernetes\n",
    "from kfp.components import load_component_from_file\n",
    "\n",
    "@dsl.pipeline(name=\"luckyvicky-finetuning\")\n",
    "def finetuning_serving_pipeline(\n",
    "    train_data_url:str=\"https://drive.google.com/uc?id=1YHU91AS53lIqoSF8NUXxZV-Rnd2vHH_6\",\n",
    "    eval_data_url:str=\"https://drive.google.com/uc?id=1loRPD53t6K0kdMRa6lv7qRO5rzDLanRR\",\n",
    "    training_code_url:str=\"https://github.com/questcollector/llmops-manifests/raw/refs/heads/main/script/sft-train.py\",\n",
    "    deepspeed_config_url:str=\"https://github.com/questcollector/llmops-manifests/raw/refs/heads/main/script/deepspeed.json\",\n",
    "    deepspeed_config_extras:str=\"{}\",\n",
    "    base_image:str=\"asia-northeast3-docker.pkg.dev/tokyo-data-452811-t2/llmops/transformers-deepspeed:v4.49.0-v0.16.4-24.06\",\n",
    "    model_id:str=\"unsloth/Meta-Llama-3.1-8B-Instruct\",\n",
    "    job_name:str=\"luckyvicky\",\n",
    "    batch_size:int=4,\n",
    "    num_train_epochs:int=10,\n",
    "    master_replica:int=1,\n",
    "    worker_replica:int=1,\n",
    "    requests_resources:str='{\"cpu\":\"2\", \"memory\": \"20Gi\", \"nvidia.com/gpu\": \"1\"}',\n",
    "    limits_resources:str='{\"cpu\":\"4\", \"memory\": \"55Gi\", \"nvidia.com/gpu\": \"1\"}',\n",
    "):\n",
    "    \"\"\"finetuning with huggingface TRL SFTTrainer\n",
    "\n",
    "    Args:\n",
    "        train_data_url: url of train dataset in jsonl file.\n",
    "        eval_data_url: url of evaluation dataset in jsonl file.\n",
    "        training_code_url: url of training code.\n",
    "        deepspeed_config_url: url of deepspeed config in json format.\n",
    "        deepspeed_config_extras: extra updates of key, value for deepspeed config in json format.\n",
    "        base_image: base image for pytorchjob.\n",
    "        model_id: pretrained model for finetuning.\n",
    "        job_name: prefix of pytorchjob/output model\n",
    "        batch_size: batch size as training argument.\n",
    "        num_train_epochs: number of training epochs as training argument.\n",
    "        master_replica: replica size for pytorchjob master nodes\n",
    "        worker_replica: replica size for pytorchjob worker nodes\n",
    "        requests_resources: training job resource requests in json format\n",
    "        limits_resources: training job resource limits in json format\n",
    "\n",
    "    Returns:\n",
    "        \n",
    "    \"\"\"\n",
    "    # create rwx PVC\n",
    "    pvc = kubernetes.CreatePVC(\n",
    "        # can also use pvc_name instead of pvc_name_suffix to use a pre-existing PVC\n",
    "        pvc_name='luckyvicky-finetuning-pvc-rwx',\n",
    "        access_modes=['ReadWriteMany'],\n",
    "        size='1024Gi',\n",
    "        storage_class_name='filestore-rwx',\n",
    "    )\n",
    "    pvc.set_caching_options(True)\n",
    "    \n",
    "    mount_path = '/train'\n",
    "    \n",
    "    # get datasets\n",
    "    \n",
    "    train_dataset = save_jsonl(\n",
    "        jsonl_url=train_data_url\n",
    "    )\n",
    "    validated_train_dataset = validate_dataset(\n",
    "        jsonl_dataset=train_dataset.output\n",
    "    )\n",
    "\n",
    "    eval_dataset = save_jsonl(\n",
    "        jsonl_url=eval_data_url\n",
    "    )\n",
    "    validated_eval_dataset = validate_dataset(\n",
    "        jsonl_dataset=eval_dataset.output\n",
    "    )\n",
    "    \n",
    "    # fetch training code\n",
    "    train_func = save_train_func(training_code_url=training_code_url)\n",
    "    \n",
    "    # fetch deepspeed config\n",
    "    deepspeed = json_to_dict(\n",
    "        json_url=deepspeed_config_url,\n",
    "        extras_to_update=deepspeed_config_extras\n",
    "    )\n",
    "\n",
    "    # generate job script\n",
    "    job_id_and_script = generate_pytorchjob_script(\n",
    "        job_name=job_name,\n",
    "        training_code=train_func.output,\n",
    "        deepspeed_config=deepspeed.output,\n",
    "        model_id=model_id,\n",
    "        train_dataset=validated_train_dataset.output,\n",
    "        eval_dataset=validated_eval_dataset.output,\n",
    "        mount_path=mount_path,\n",
    "        batch_size=batch_size,\n",
    "        num_train_epochs=num_train_epochs\n",
    "    )\n",
    "    \n",
    "    # create pytorchjob and tracing status/logs\n",
    "    pytorchjob = run_pytorchjob(\n",
    "        job_name=job_name,\n",
    "        job_id=job_id_and_script.outputs['job_id'],\n",
    "        base_image=base_image,\n",
    "        pvc_name=pvc.outputs['name'],\n",
    "        mount_path=mount_path,\n",
    "        exec_script=job_id_and_script.outputs['exec_script'],\n",
    "        master_replica=master_replica,\n",
    "        worker_replica=worker_replica,\n",
    "        requests_resources=requests_resources,\n",
    "        limits_resources=limits_resources,\n",
    "    )\n",
    "    isvc = run_isvc(\n",
    "        pvc_name=pvc.outputs['name'],\n",
    "        model_id=model_id,\n",
    "        pytorchjob_name=pytorchjob.output,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7dcad247",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp.compiler import Compiler\n",
    "\n",
    "Compiler().compile(finetuning_serving_pipeline, package_path=\"luckyvicky-finetuning-serving.yaml\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
